# Word2Vec PyTorch ä»£ç è¯¦ç»†è§£è¯»

## ä¸€ã€å¯¼å…¥å’ŒåŸºç¡€è®¾ç½®

```python
import torch  # PyTorch ä¸»åº“
import torch.nn as nn  # ç¥ç»ç½‘ç»œæ¨¡å—
import torch.optim as optim  # ä¼˜åŒ–å™¨æ¨¡å—
```

**è§£é‡Š**ï¼š
- `torch`: PyTorch æ ¸å¿ƒåº“ï¼Œæä¾›å¼ é‡è¿ç®—ã€è‡ªåŠ¨æ±‚å¯¼ç­‰åŠŸèƒ½
- `torch.nn`: ç¥ç»ç½‘ç»œæ¨¡å—ï¼ŒåŒ…å«å„ç§å±‚ï¼ˆå¦‚ Embeddingã€Linear ç­‰ï¼‰
- `torch.optim`: ä¼˜åŒ–å™¨æ¨¡å—ï¼ŒåŒ…å« Adamã€SGD ç­‰ä¼˜åŒ–ç®—æ³•

---

## äºŒã€Word2Vec æ¨¡å‹ç±»è¯¦è§£

### 1. `__init__` æ–¹æ³•ï¼ˆç¬¬ 55-62 è¡Œï¼‰

```python
def __init__(self, vocab_size, embedding_dim):
    super(Word2Vec, self).__init__()
    self.center_embed = nn.Embedding(vocab_size, embedding_dim)
    self.outside_embed = nn.Embedding(vocab_size, embedding_dim)
    self.center_embed.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)
    self.outside_embed.weight.data.uniform_(-0.5 / embedding_dim, 0.5 / embedding_dim)
```

**é€è¡Œè§£é‡Š**ï¼š

#### `super(Word2Vec, self).__init__()`
- è°ƒç”¨çˆ¶ç±» `nn.Module` çš„åˆå§‹åŒ–æ–¹æ³•
- è®©æ¨¡å‹èƒ½å¤Ÿæ³¨å†Œå‚æ•°ã€ä½¿ç”¨ GPUã€ä¿å­˜/åŠ è½½æ¨¡å‹ç­‰

#### `nn.Embedding(vocab_size, embedding_dim)`
**è¿™æ˜¯æ ¸å¿ƒï¼Embedding å±‚åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ**

- **ä½œç”¨**ï¼šå°†è¯çš„ç´¢å¼•ï¼ˆæ•´æ•° IDï¼‰æ˜ å°„ä¸ºç¨ å¯†å‘é‡ï¼ˆè¯å‘é‡ï¼‰
- **å†…éƒ¨ç»“æ„**ï¼šå®é™…ä¸Šæ˜¯ä¸€ä¸ªæŸ¥æ‰¾è¡¨ï¼ˆLookup Tableï¼‰
  - å½¢çŠ¶ï¼š`(vocab_size, embedding_dim)`
  - ä¾‹å¦‚ï¼šå¦‚æœ `vocab_size=1000`, `embedding_dim=32`
  - é‚£ä¹ˆ Embedding å±‚å°±æ˜¯ä¸€ä¸ª `1000 Ã— 32` çš„çŸ©é˜µ
  - æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè¯çš„å‘é‡è¡¨ç¤º

**å…·ä½“ä¾‹å­**ï¼š
```python
# å‡è®¾è¯æ±‡è¡¨å¤§å°æ˜¯ 1000ï¼ŒåµŒå…¥ç»´åº¦æ˜¯ 32
embedding = nn.Embedding(1000, 32)

# è¾“å…¥ï¼šè¯çš„ç´¢å¼• [5, 10, 23]ï¼ˆè¡¨ç¤ºç¬¬ 5ã€10ã€23 ä¸ªè¯ï¼‰
input_ids = torch.tensor([5, 10, 23])

# è¾“å‡ºï¼šå½¢çŠ¶ä¸º (3, 32) çš„å¼ é‡
# ç¬¬ 0 è¡Œæ˜¯ç¬¬ 5 ä¸ªè¯çš„å‘é‡
# ç¬¬ 1 è¡Œæ˜¯ç¬¬ 10 ä¸ªè¯çš„å‘é‡  
# ç¬¬ 2 è¡Œæ˜¯ç¬¬ 23 ä¸ªè¯çš„å‘é‡
output = embedding(input_ids)  # shape: (3, 32)
```

**ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ª Embedding å±‚ï¼Ÿ**
- `center_embed`: ç”¨äºä¸­å¿ƒè¯çš„å‘é‡è¡¨ç¤ºï¼ˆè¾“å…¥è¯ï¼‰
- `outside_embed`: ç”¨äºä¸Šä¸‹æ–‡è¯çš„å‘é‡è¡¨ç¤ºï¼ˆè¾“å‡ºè¯ï¼‰
- è¿™æ˜¯ Word2Vec çš„ç»å…¸è®¾è®¡ï¼Œä¸¤ä¸ªçŸ©é˜µå¯ä»¥å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºï¼Œè®­ç»ƒæ›´ç¨³å®š

#### `weight.data.uniform_(a, b)`
- `weight`: Embedding å±‚çš„æƒé‡çŸ©é˜µï¼ˆå°±æ˜¯é‚£ä¸ªæŸ¥æ‰¾è¡¨ï¼‰
- `data`: è®¿é—®å‚æ•°çš„å®é™…æ•°å€¼ï¼ˆä¸è¿½è¸ªæ¢¯åº¦ï¼‰
- `uniform_(a, b)`: ç”¨å‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–ï¼ŒèŒƒå›´æ˜¯ `[a, b)`
- `-0.5 / embedding_dim` åˆ° `0.5 / embedding_dim`: å°èŒƒå›´åˆå§‹åŒ–ï¼Œé¿å…åˆå§‹å€¼è¿‡å¤§

**åˆå§‹åŒ–ç¤ºä¾‹**ï¼š
```python
# å¦‚æœ embedding_dim = 32
# åˆå§‹åŒ–èŒƒå›´ï¼š[-0.5/32, 0.5/32] = [-0.015625, 0.015625]
# è¿™æ ·åˆå§‹å‘é‡éƒ½å¾ˆå°ï¼Œè®­ç»ƒæ›´ç¨³å®š
```

---

### 2. `forward` æ–¹æ³•ï¼ˆç¬¬ 64-73 è¡Œï¼‰- **æ ¸å¿ƒå‰å‘ä¼ æ’­**

```python
def forward(self, center_words, context_words, negative_samples):
    v_c = self.center_embed(center_words)
    u_o = self.outside_embed(context_words)
    u_neg = self.outside_embed(negative_samples)
    positive_scores = torch.sum(u_o * v_c, dim=1)
    negative_scores = torch.bmm(v_c.unsqueeze(1), u_neg.transpose(1, 2)).squeeze(1)
    return positive_scores, negative_scores
```

**é€è¡Œè¯¦ç»†è§£é‡Š**ï¼š

#### è¾“å…¥å‚æ•°è¯´æ˜
å‡è®¾æ‰¹æ¬¡å¤§å° `B=1`ï¼ˆä»£ç ä¸­æ¯æ¬¡å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼‰ï¼š
- `center_words`: å½¢çŠ¶ `(1,)`ï¼Œä¾‹å¦‚ `[5]` è¡¨ç¤ºç¬¬ 5 ä¸ªè¯ä½œä¸ºä¸­å¿ƒè¯
- `context_words`: å½¢çŠ¶ `(1,)`ï¼Œä¾‹å¦‚ `[10]` è¡¨ç¤ºç¬¬ 10 ä¸ªè¯ä½œä¸ºä¸Šä¸‹æ–‡è¯
- `negative_samples`: å½¢çŠ¶ `(1, k)`ï¼Œä¾‹å¦‚ `[[20, 30, 40, 50, 60]]`ï¼Œk=5 ä¸ªè´Ÿæ ·æœ¬

#### ç¬¬ 66 è¡Œï¼š`v_c = self.center_embed(center_words)`
**Embedding ä¹‹åå¾—åˆ°äº†ä»€ä¹ˆï¼Ÿ**

```python
# è¾“å…¥ï¼šcenter_words = [5]  (å½¢çŠ¶: (1,))
# Embedding å±‚æŸ¥æ‰¾ï¼šå–å‡ºç¬¬ 5 è¡Œçš„å‘é‡
# è¾“å‡ºï¼šv_c å½¢çŠ¶ä¸º (1, 32)
# v_c[0] å°±æ˜¯ç¬¬ 5 ä¸ªè¯çš„ 32 ç»´å‘é‡è¡¨ç¤º
```

**å…·ä½“è¿‡ç¨‹**ï¼š
1. `center_words = [5]` æ˜¯ç´¢å¼•
2. Embedding å±‚å†…éƒ¨æœ‰ä¸€ä¸ªçŸ©é˜µ `W_center`ï¼Œå½¢çŠ¶ `(vocab_size, 32)`
3. `W_center[5]` å°±æ˜¯ç¬¬ 5 ä¸ªè¯çš„å‘é‡ï¼ˆ32 ç»´ï¼‰
4. è¿”å›è¿™ä¸ªå‘é‡ï¼š`v_c = [[w1, w2, ..., w32]]`ï¼Œå½¢çŠ¶ `(1, 32)`

**å¯è§†åŒ–**ï¼š
```
Embedding çŸ©é˜µ (vocab_size Ã— 32):
è¡Œ0: [0.1, 0.2, ..., 0.3]
è¡Œ1: [0.4, 0.1, ..., 0.2]
...
è¡Œ5: [0.5, 0.3, ..., 0.1]  â† è¿™å°±æ˜¯ v_c[0]
...
è¡Œ999: [0.2, 0.4, ..., 0.5]

è¾“å…¥ç´¢å¼• [5] â†’ è¾“å‡ºå‘é‡ [0.5, 0.3, ..., 0.1] (32ç»´)
```

#### ç¬¬ 67 è¡Œï¼š`u_o = self.outside_embed(context_words)`
- åŒæ ·è¿‡ç¨‹ï¼Œä» `outside_embed` çŸ©é˜µä¸­å–å‡ºä¸Šä¸‹æ–‡è¯çš„å‘é‡
- `u_o` å½¢çŠ¶ï¼š`(1, 32)`

#### ç¬¬ 68 è¡Œï¼š`u_neg = self.outside_embed(negative_samples)`
- è¾“å…¥ï¼š`negative_samples = [[20, 30, 40, 50, 60]]`ï¼Œå½¢çŠ¶ `(1, 5)`
- Embedding æŸ¥æ‰¾ï¼šå–å‡ºç¬¬ 20ã€30ã€40ã€50ã€60 è¡Œçš„å‘é‡
- è¾“å‡ºï¼š`u_neg` å½¢çŠ¶ `(1, 5, 32)`
  - `u_neg[0, 0]` æ˜¯ç¬¬ 20 ä¸ªè¯çš„å‘é‡
  - `u_neg[0, 1]` æ˜¯ç¬¬ 30 ä¸ªè¯çš„å‘é‡
  - ...ä»¥æ­¤ç±»æ¨

#### ç¬¬ 70 è¡Œï¼š`positive_scores = torch.sum(u_o * v_c, dim=1)`
**è®¡ç®—æ­£æ ·æœ¬åˆ†æ•°ï¼ˆä¸­å¿ƒè¯å’Œä¸Šä¸‹æ–‡è¯çš„ç›¸ä¼¼åº¦ï¼‰**

**æ­¥éª¤åˆ†è§£**ï¼š
1. `u_o * v_c`: é€å…ƒç´ ç›¸ä¹˜ï¼ˆHadamard ç§¯ï¼‰
   - `u_o`: `(1, 32)`
   - `v_c`: `(1, 32)`
   - ç»“æœï¼š`(1, 32)`ï¼Œæ¯ä¸ªä½ç½®å¯¹åº”ç›¸ä¹˜

2. `torch.sum(..., dim=1)`: åœ¨ç¬¬ 1 ç»´ï¼ˆåˆ—ï¼‰ä¸Šæ±‚å’Œ
   - ç»“æœï¼š`(1,)`ï¼Œå¾—åˆ°ä¸€ä¸ªæ ‡é‡åˆ†æ•°
   - **è¿™å°±æ˜¯ç‚¹ç§¯ï¼ˆå†…ç§¯ï¼‰ï¼** è¡¡é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦

**æ•°å­¦å«ä¹‰**ï¼š
```
positive_scores = u_o Â· v_c = Î£(u_o[i] * v_c[i])
```
- åˆ†æ•°è¶Šå¤§ â†’ ä¸¤ä¸ªè¯å‘é‡è¶Šç›¸ä¼¼ â†’ è¶Šå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡å…³ç³»
- åˆ†æ•°è¶Šå° â†’ ä¸¤ä¸ªè¯å‘é‡è¶Šä¸ç›¸ä¼¼

**ç¤ºä¾‹**ï¼š
```python
v_c = [[0.5, 0.3, 0.1, ..., 0.2]]  # ä¸­å¿ƒè¯å‘é‡
u_o = [[0.4, 0.2, 0.3, ..., 0.1]]  # ä¸Šä¸‹æ–‡è¯å‘é‡

# é€å…ƒç´ ç›¸ä¹˜
element_wise = [[0.5*0.4, 0.3*0.2, 0.1*0.3, ..., 0.2*0.1]]
             = [[0.2, 0.06, 0.03, ..., 0.02]]

# æ±‚å’Œ
positive_scores = [0.2 + 0.06 + 0.03 + ... + 0.02] = [2.5]
```

#### ç¬¬ 72 è¡Œï¼š`negative_scores = torch.bmm(v_c.unsqueeze(1), u_neg.transpose(1, 2)).squeeze(1)`
**è®¡ç®—è´Ÿæ ·æœ¬åˆ†æ•°ï¼ˆä¸­å¿ƒè¯å’Œè´Ÿæ ·æœ¬è¯çš„ç›¸ä¼¼åº¦ï¼‰**

**æ­¥éª¤åˆ†è§£**ï¼š

1. `v_c.unsqueeze(1)`:
   - è¾“å…¥ï¼š`v_c` å½¢çŠ¶ `(1, 32)`
   - åœ¨ç¬¬ 1 ç»´æ’å…¥ä¸€ä¸ªç»´åº¦
   - è¾“å‡ºï¼š`(1, 1, 32)`
   - **ç›®çš„**ï¼šä¸ºæ‰¹é‡çŸ©é˜µä¹˜æ³•å‡†å¤‡

2. `u_neg.transpose(1, 2)`:
   - è¾“å…¥ï¼š`u_neg` å½¢çŠ¶ `(1, 5, 32)`
   - äº¤æ¢ç»´åº¦ 1 å’Œ 2
   - è¾“å‡ºï¼š`(1, 32, 5)`
   - **ç›®çš„**ï¼šè½¬ç½®ï¼Œè®©çŸ©é˜µä¹˜æ³•ç»´åº¦åŒ¹é…

3. `torch.bmm(...)`:
   - æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆBatch Matrix Multiplicationï¼‰
   - è¾“å…¥1ï¼š`(1, 1, 32)`
   - è¾“å…¥2ï¼š`(1, 32, 5)`
   - è¾“å‡ºï¼š`(1, 1, 5)`
   - **è®¡ç®—**ï¼š`[1Ã—32] Ã— [32Ã—5] = [1Ã—5]`
   - ç»“æœï¼š5 ä¸ªè´Ÿæ ·æœ¬ä¸ä¸­å¿ƒè¯çš„ç‚¹ç§¯åˆ†æ•°

4. `.squeeze(1)`:
   - è¾“å…¥ï¼š`(1, 1, 5)`
   - ç§»é™¤ç»´åº¦ 1ï¼ˆé•¿åº¦ä¸º 1 çš„ç»´åº¦ï¼‰
   - è¾“å‡ºï¼š`(1, 5)`

**æ•°å­¦å«ä¹‰**ï¼š
```python
# negative_scores[0, 0] = v_c Â· u_neg[0, 0]  (ä¸­å¿ƒè¯ vs ç¬¬1ä¸ªè´Ÿæ ·æœ¬)
# negative_scores[0, 1] = v_c Â· u_neg[0, 1]  (ä¸­å¿ƒè¯ vs ç¬¬2ä¸ªè´Ÿæ ·æœ¬)
# ...
# negative_scores[0, 4] = v_c Â· u_neg[0, 4]  (ä¸­å¿ƒè¯ vs ç¬¬5ä¸ªè´Ÿæ ·æœ¬)
```

**å¯è§†åŒ–**ï¼š
```
v_c:        [1Ã—32]
u_neg:      [1Ã—5Ã—32] è½¬ç½®å â†’ [1Ã—32Ã—5]

bmm è®¡ç®—ï¼š
[1Ã—32] Ã— [32Ã—5] = [1Ã—5]

ç»“æœï¼šnegative_scores = [[score1, score2, score3, score4, score5]]
```

---

## ä¸‰ã€æŸå¤±å‡½æ•°è¯¦è§£ï¼ˆç¬¬ 146-150 è¡Œï¼‰

```python
def loss_fn(positive_scores, negative_scores):
    positive_loss = -torch.log(torch.sigmoid(positive_scores) + 1e-8)
    negative_loss = -torch.sum(torch.log(torch.sigmoid(-negative_scores) + 1e-8), dim=1)
    return torch.mean(positive_loss + negative_loss)
```

**é€è¡Œè§£é‡Š**ï¼š

#### `torch.sigmoid(positive_scores)`
- **Sigmoid å‡½æ•°**ï¼šå°†ä»»æ„å®æ•°æ˜ å°„åˆ° `(0, 1)` åŒºé—´
- å…¬å¼ï¼š`sigmoid(x) = 1 / (1 + e^(-x))`
- **ä½œç”¨**ï¼šå°†ç‚¹ç§¯åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡
  - åˆ†æ•°å¤§ â†’ æ¥è¿‘ 1ï¼ˆé«˜æ¦‚ç‡æ˜¯ä¸Šä¸‹æ–‡ï¼‰
  - åˆ†æ•°å° â†’ æ¥è¿‘ 0ï¼ˆä½æ¦‚ç‡æ˜¯ä¸Šä¸‹æ–‡ï¼‰

**ç¤ºä¾‹**ï¼š
```python
positive_scores = [2.5]  # ç‚¹ç§¯åˆ†æ•°
sigmoid(2.5) â‰ˆ 0.924  # è½¬æ¢ä¸ºæ¦‚ç‡ï¼š92.4% çš„æ¦‚ç‡æ˜¯ä¸Šä¸‹æ–‡
```

#### `torch.log(... + 1e-8)`
- `log`: è‡ªç„¶å¯¹æ•°
- `1e-8`: é˜²æ­¢ log(0) å¯¼è‡´æ•°å€¼ä¸ç¨³å®š
- **ä½œç”¨**ï¼šè®¡ç®—å¯¹æ•°æ¦‚ç‡ï¼ˆç”¨äºäº¤å‰ç†µæŸå¤±ï¼‰

#### `-torch.log(...)`
- è´Ÿå·ï¼šå› ä¸ºæˆ‘ä»¬è¦æœ€å¤§åŒ–æ¦‚ç‡ï¼Œç­‰ä»·äºæœ€å°åŒ–è´Ÿå¯¹æ•°æ¦‚ç‡
- **ç›®æ ‡**ï¼šè®© `sigmoid(positive_scores)` æ¥è¿‘ 1ï¼Œå³ `-log(æ¥è¿‘1)` æ¥è¿‘ 0

#### `torch.sigmoid(-negative_scores)`
- **å…³é”®**ï¼šå¯¹è´Ÿæ ·æœ¬åˆ†æ•°å–è´Ÿå·
- **å«ä¹‰**ï¼šæˆ‘ä»¬å¸Œæœ›è´Ÿæ ·æœ¬åˆ†æ•°å°ï¼ˆä¸ç›¸ä¼¼ï¼‰
- `sigmoid(-negative_scores)` è¡¨ç¤º"ä¸æ˜¯ä¸Šä¸‹æ–‡"çš„æ¦‚ç‡
- æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªæ¦‚ç‡å¤§ï¼ˆæ¥è¿‘ 1ï¼‰

**ç¤ºä¾‹**ï¼š
```python
negative_scores = [[1.0, 0.5, -0.5, -1.0, -2.0]]
# æˆ‘ä»¬å¸Œæœ›è¿™äº›åˆ†æ•°éƒ½å°ï¼ˆä¸ç›¸ä¼¼ï¼‰

sigmoid(-negative_scores) = sigmoid([[-1.0, -0.5, 0.5, 1.0, 2.0]])
                          â‰ˆ [[0.27, 0.38, 0.62, 0.73, 0.88]]
# è¿™äº›å€¼æ¥è¿‘ 1 è¡¨ç¤º"ä¸æ˜¯ä¸Šä¸‹æ–‡"çš„æ¦‚ç‡é«˜ âœ“
```

#### `torch.sum(..., dim=1)`
- å¯¹ 5 ä¸ªè´Ÿæ ·æœ¬çš„æŸå¤±æ±‚å’Œ
- å½¢çŠ¶ï¼š`(1, 5)` â†’ `(1,)`

#### `torch.mean(positive_loss + negative_loss)`
- å°†æ­£æ ·æœ¬æŸå¤±å’Œè´Ÿæ ·æœ¬æŸå¤±ç›¸åŠ ï¼Œç„¶åæ±‚å¹³å‡
- è¿”å›ä¸€ä¸ªæ ‡é‡ï¼ˆå•ä¸ªæ•°å€¼ï¼‰

**æŸå¤±å‡½æ•°çš„ç›®æ ‡**ï¼š
- **æœ€å°åŒ–** `positive_loss`ï¼šè®©æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦æœ€å¤§åŒ–
- **æœ€å°åŒ–** `negative_loss`ï¼šè®©è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦æœ€å°åŒ–
- **æ€»ä½“**ï¼šè®©æ¨¡å‹å­¦ä¼šåŒºåˆ†çœŸæ­£çš„ä¸Šä¸‹æ–‡è¯å’Œéšæœºè¯

---

## å››ã€è®­ç»ƒæµç¨‹è¶…è¯¦ç»†è§£è¯»ï¼ˆç¬¬ 88-112 è¡Œï¼‰

è¿™æ˜¯æ•´ä¸ª Word2Vec æ¨¡å‹çš„æ ¸å¿ƒè®­ç»ƒå¾ªç¯ã€‚è®©æˆ‘ä»¬é€è¡Œæ·±å…¥ç†è§£ï¼Œç‰¹åˆ«æ˜¯ **PyTorch å¦‚ä½•è‡ªåŠ¨æ±‚è§£æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°**ï¼Œä»¥åŠ **ä¸¤ä¸ª Embedding å±‚å¦‚ä½•ååŒå·¥ä½œ**ã€‚

```python
def train(model, training_data, optimizer, loss_fn, epochs, negative_sampler, k_negative_samples=5):
    for epoch in range(epochs):
        total_loss = 0
        random.shuffle(training_data)
        progress_bar = tqdm(training_data, desc=f"Epoch {epoch + 1}/{epochs}")
        for i, (center_word_idx, context_word_idx) in enumerate(progress_bar):
            # æ­¥éª¤1ï¼šå‡†å¤‡è¾“å…¥æ•°æ®
            center_tensor = torch.tensor([center_word_idx], dtype=torch.long)
            context_tensor = torch.tensor([context_word_idx], dtype=torch.long)
            neg_sample_indices = negative_sampler.get_negative_samples(context_word_idx, k_negative_samples)
            neg_tensor = torch.tensor([neg_sample_indices], dtype=torch.long)

            # æ­¥éª¤2-5ï¼šè®­ç»ƒçš„æ ¸å¿ƒæ­¥éª¤
            optimizer.zero_grad()  # æ­¥éª¤2ï¼šæ¢¯åº¦æ¸…é›¶
            positive_scores, negative_scores = model(center_tensor, context_tensor, neg_tensor)  # æ­¥éª¤3ï¼šå‰å‘ä¼ æ’­
            loss = loss_fn(positive_scores, negative_scores)  # æ­¥éª¤4ï¼šè®¡ç®—æŸå¤±
            loss.backward()  # æ­¥éª¤5ï¼šåå‘ä¼ æ’­ï¼ˆè‡ªåŠ¨æ±‚å¯¼ï¼‰
            optimizer.step()  # æ­¥éª¤6ï¼šæ›´æ–°å‚æ•°
            total_loss += loss.item()
```

---

### æ­¥éª¤1ï¼šå‡†å¤‡è¾“å…¥æ•°æ®ï¼ˆç¬¬ 94-98 è¡Œï¼‰

#### `center_tensor = torch.tensor([center_word_idx], dtype=torch.long)`
**ä½œç”¨**ï¼šå°† Python æ•´æ•°è½¬æ¢ä¸º PyTorch å¼ é‡

**è¯¦ç»†è¯´æ˜**ï¼š
- `torch.tensor()`: åˆ›å»ºå¼ é‡
- `dtype=torch.long`: 64 ä½æ•´æ•°ç±»å‹ï¼ŒEmbedding å±‚è¦æ±‚ç´¢å¼•å¿…é¡»æ˜¯æ•´æ•°
- `[center_word_idx]`: ç”¨åˆ—è¡¨åŒ…è£…ï¼Œä¿æŒæ‰¹æ¬¡ç»´åº¦

**ç¤ºä¾‹**ï¼š
```python
center_word_idx = 5  # Python æ•´æ•°
center_tensor = torch.tensor([5], dtype=torch.long)  # PyTorch å¼ é‡
# å½¢çŠ¶ï¼š(1,)ï¼Œå€¼ï¼š[5]
```

**ä¸ºä»€ä¹ˆéœ€è¦æ‰¹æ¬¡ç»´åº¦ï¼Ÿ**
- å³ä½¿åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿè¦ä¿æŒ `(1,)` çš„å½¢çŠ¶
- è¿™æ ·ä»£ç å¯ä»¥è½»æ¾æ‰©å±•åˆ°æ‰¹é‡è®­ç»ƒï¼ˆå¦‚ `(32,)` è¡¨ç¤º 32 ä¸ªæ ·æœ¬ï¼‰

#### `neg_tensor = torch.tensor([neg_sample_indices], dtype=torch.long)`
**ç¤ºä¾‹**ï¼š
```python
neg_sample_indices = [20, 30, 40, 50, 60]  # 5 ä¸ªè´Ÿæ ·æœ¬ç´¢å¼•
neg_tensor = torch.tensor([[20, 30, 40, 50, 60]], dtype=torch.long)
# å½¢çŠ¶ï¼š(1, 5)
```

---

### æ­¥éª¤2ï¼šæ¢¯åº¦æ¸…é›¶ï¼ˆç¬¬ 101 è¡Œï¼‰- `optimizer.zero_grad()`

**è¿™æ˜¯å…³é”®çš„ç¬¬ä¸€æ­¥ï¼**

#### ä¸ºä»€ä¹ˆéœ€è¦æ¢¯åº¦æ¸…é›¶ï¼Ÿ

**PyTorch çš„æ¢¯åº¦ç´¯ç§¯æœºåˆ¶**ï¼š
- PyTorch é»˜è®¤ä¼š**ç´¯ç§¯æ¢¯åº¦**ï¼ˆç´¯åŠ ï¼‰
- å¦‚æœä¸æ¸…é›¶ï¼Œæ¯æ¬¡ `backward()` çš„æ¢¯åº¦ä¼šå åŠ åˆ°ä¹‹å‰çš„æ¢¯åº¦ä¸Š
- è¿™ä¼šå¯¼è‡´æ¢¯åº¦è¶Šæ¥è¶Šå¤§ï¼Œè®­ç»ƒä¸ç¨³å®š

**å†…éƒ¨æœºåˆ¶**ï¼š
```python
# PyTorch å†…éƒ¨å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
def zero_grad(self):
    for param in self.parameters():
        if param.grad is not None:
            param.grad.zero_()  # å°†æ¢¯åº¦ç½®ä¸º 0
```

**å…·ä½“è¿‡ç¨‹**ï¼š
```python
# å‡è®¾æ¨¡å‹æœ‰ä¸¤ä¸ª Embedding å±‚ï¼Œæ¯ä¸ªæœ‰ 1000Ã—32 ä¸ªå‚æ•°
# center_embed.weight: (1000, 32) - 32,000 ä¸ªå‚æ•°
# outside_embed.weight: (1000, 32) - 32,000 ä¸ªå‚æ•°

# zero_grad() ä¼šï¼š
# 1. éå†æ‰€æœ‰å‚æ•°
# 2. å°†æ¯ä¸ªå‚æ•°çš„ .grad å±æ€§ç½®ä¸º 0
# center_embed.weight.grad = zeros(1000, 32)
# outside_embed.weight.grad = zeros(1000, 32)
```

**ä¸æ¸…é›¶çš„åæœ**ï¼š
```python
# ç¬¬ä¸€æ¬¡è¿­ä»£
loss1.backward()  # è®¡ç®—æ¢¯åº¦ï¼Œå‡è®¾ grad = [0.1, 0.2, ...]

# ç¬¬äºŒæ¬¡è¿­ä»£ï¼ˆæ²¡æœ‰ zero_gradï¼‰
loss2.backward()  # æ¢¯åº¦ä¼šç´¯åŠ ï¼šgrad = [0.1+0.15, 0.2+0.25, ...]
                  # æ¢¯åº¦è¶Šæ¥è¶Šå¤§ï¼Œè®­ç»ƒçˆ†ç‚¸ï¼
```

---

### æ­¥éª¤3ï¼šå‰å‘ä¼ æ’­ï¼ˆç¬¬ 102 è¡Œï¼‰- `model(...)`

**è¿™æ˜¯ä¸¤ä¸ª Embedding å±‚ååŒå·¥ä½œçš„æ ¸å¿ƒï¼**

#### è°ƒç”¨è¿‡ç¨‹

```python
positive_scores, negative_scores = model(center_tensor, context_tensor, neg_tensor)
# ç­‰ä»·äºï¼š
positive_scores, negative_scores = model.forward(center_tensor, context_tensor, neg_tensor)
```

#### ä¸¤ä¸ª Embedding å±‚å¦‚ä½•ååŒå·¥ä½œï¼Ÿ

**å…³é”®ç†è§£**ï¼šä¸¤ä¸ª Embedding å±‚**å…±äº«ç›¸åŒçš„è¯æ±‡è¡¨**ï¼Œä½†**å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºè§’åº¦**ã€‚

**å…·ä½“è¿‡ç¨‹**ï¼š

```python
# è¾“å…¥ç¤ºä¾‹
center_tensor = [5]        # ä¸­å¿ƒè¯ï¼šç¬¬ 5 ä¸ªè¯ï¼ˆæ¯”å¦‚ "cat"ï¼‰
context_tensor = [10]      # ä¸Šä¸‹æ–‡è¯ï¼šç¬¬ 10 ä¸ªè¯ï¼ˆæ¯”å¦‚ "dog"ï¼‰
neg_tensor = [[20, 30, 40, 50, 60]]  # è´Ÿæ ·æœ¬ï¼š5 ä¸ªéšæœºè¯

# æ­¥éª¤ 3.1ï¼šcenter_embed æŸ¥æ‰¾ä¸­å¿ƒè¯å‘é‡
v_c = self.center_embed(center_tensor)
# ä» center_embed çŸ©é˜µçš„ç¬¬ 5 è¡Œå–å‡ºå‘é‡
# v_c å½¢çŠ¶ï¼š(1, 32)
# v_c = [[0.5, 0.3, 0.1, ..., 0.2]]  # "cat" çš„ä¸­å¿ƒè¯å‘é‡

# æ­¥éª¤ 3.2ï¼šoutside_embed æŸ¥æ‰¾ä¸Šä¸‹æ–‡è¯å‘é‡
u_o = self.outside_embed(context_tensor)
# ä» outside_embed çŸ©é˜µçš„ç¬¬ 10 è¡Œå–å‡ºå‘é‡
# u_o å½¢çŠ¶ï¼š(1, 32)
# u_o = [[0.4, 0.2, 0.3, ..., 0.1]]  # "dog" çš„ä¸Šä¸‹æ–‡è¯å‘é‡

# æ­¥éª¤ 3.3ï¼šoutside_embed æŸ¥æ‰¾è´Ÿæ ·æœ¬å‘é‡
u_neg = self.outside_embed(neg_tensor)
# ä» outside_embed çŸ©é˜µçš„ç¬¬ 20, 30, 40, 50, 60 è¡Œå–å‡ºå‘é‡
# u_neg å½¢çŠ¶ï¼š(1, 5, 32)
# u_neg[0, 0] = outside_embed[20]  # ç¬¬ 1 ä¸ªè´Ÿæ ·æœ¬çš„å‘é‡
# u_neg[0, 1] = outside_embed[30]  # ç¬¬ 2 ä¸ªè´Ÿæ ·æœ¬çš„å‘é‡
# ...

# æ­¥éª¤ 3.4ï¼šè®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°
positive_scores = torch.sum(u_o * v_c, dim=1)
# = torch.sum([[0.4*0.5, 0.2*0.3, 0.3*0.1, ..., 0.1*0.2]], dim=1)
# = [0.2 + 0.06 + 0.03 + ... + 0.02]
# = [2.5]  # "cat" å’Œ "dog" çš„ç›¸ä¼¼åº¦åˆ†æ•°

negative_scores = torch.bmm(v_c.unsqueeze(1), u_neg.transpose(1, 2)).squeeze(1)
# è®¡ç®—ä¸­å¿ƒè¯ä¸ 5 ä¸ªè´Ÿæ ·æœ¬çš„ç›¸ä¼¼åº¦
# = [[1.0, 0.5, -0.5, -1.0, -2.0]]
```

**ä¸¤ä¸ª Embedding å±‚çš„åˆ†å·¥**ï¼š

1. **center_embedï¼ˆä¸­å¿ƒè¯åµŒå…¥ï¼‰**ï¼š
   - åªç”¨äº**è¾“å…¥è¯**ï¼ˆä¸­å¿ƒè¯ï¼‰
   - å­¦ä¹ "ä½œä¸ºä¸­å¿ƒè¯æ—¶"çš„è¡¨ç¤º
   - æœ€ç»ˆè®­ç»ƒå®Œæˆåï¼Œ**é€šå¸¸ä½¿ç”¨è¿™ä¸ªä½œä¸ºè¯å‘é‡**

2. **outside_embedï¼ˆä¸Šä¸‹æ–‡è¯åµŒå…¥ï¼‰**ï¼š
   - ç”¨äº**è¾“å‡ºè¯**ï¼ˆä¸Šä¸‹æ–‡è¯å’Œè´Ÿæ ·æœ¬ï¼‰
   - å­¦ä¹ "ä½œä¸ºä¸Šä¸‹æ–‡è¯æ—¶"çš„è¡¨ç¤º
   - å¸®åŠ©è®­ç»ƒï¼Œä½†æœ€ç»ˆä¸ä¸€å®šä½¿ç”¨

**ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªï¼Ÿ**
- **è®­ç»ƒç¨³å®šæ€§**ï¼šä¸¤ä¸ªçŸ©é˜µå¯ä»¥å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºè§’åº¦ï¼Œè®­ç»ƒæ›´ç¨³å®š
- **è¡¨è¾¾èƒ½åŠ›**ï¼šä¸€ä¸ªè¯åœ¨ä¸åŒè¯­å¢ƒä¸‹å¯èƒ½æœ‰ä¸åŒå«ä¹‰ï¼Œä¸¤ä¸ªçŸ©é˜µå¯ä»¥æ•æ‰è¿™ç§å·®å¼‚
- **ç»å…¸è®¾è®¡**ï¼šè¿™æ˜¯ Word2Vec è®ºæ–‡ä¸­çš„æ ‡å‡†è®¾è®¡

**å¯è§†åŒ–ä¸¤ä¸ª Embedding çŸ©é˜µ**ï¼š
```
center_embed çŸ©é˜µ (1000 Ã— 32):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0: [0.1, 0.2, ..., 0.3] â”‚
â”‚ 1: [0.4, 0.1, ..., 0.2] â”‚
â”‚ ...                     â”‚
â”‚ 5: [0.5, 0.3, ..., 0.1] â”‚ â† "cat" çš„ä¸­å¿ƒè¯å‘é‡
â”‚ ...                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

outside_embed çŸ©é˜µ (1000 Ã— 32):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0: [0.2, 0.1, ..., 0.4] â”‚  â† æ³¨æ„ï¼šæ•°å€¼ä¸åŒï¼
â”‚ 1: [0.3, 0.2, ..., 0.1] â”‚
â”‚ ...                     â”‚
â”‚ 10: [0.4, 0.2, ..., 0.1]â”‚ â† "dog" çš„ä¸Šä¸‹æ–‡è¯å‘é‡
â”‚ ...                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç‚¹**ï¼š
- ä¸¤ä¸ªçŸ©é˜µçš„**åŒä¸€è¡Œå¯¹åº”åŒä¸€ä¸ªè¯**ï¼ˆå¦‚ç¬¬ 5 è¡Œéƒ½æ˜¯ "cat"ï¼‰
- ä½†**æ•°å€¼ä¸åŒ**ï¼Œå› ä¸ºå®ƒä»¬å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºè§’åº¦
- è®­ç»ƒæ—¶ï¼Œä¸¤ä¸ªçŸ©é˜µ**åŒæ—¶æ›´æ–°**ï¼ŒååŒä¼˜åŒ–

---

### æ­¥éª¤4ï¼šè®¡ç®—æŸå¤±ï¼ˆç¬¬ 103 è¡Œï¼‰- `loss_fn(...)`

```python
loss = loss_fn(positive_scores, negative_scores)
```

**è¯¦ç»†è¿‡ç¨‹**ï¼š
```python
# è¾“å…¥
positive_scores = [2.5]  # æ­£æ ·æœ¬åˆ†æ•°
negative_scores = [[1.0, 0.5, -0.5, -1.0, -2.0]]  # è´Ÿæ ·æœ¬åˆ†æ•°

# è®¡ç®—æ­£æ ·æœ¬æŸå¤±
positive_loss = -torch.log(torch.sigmoid(2.5) + 1e-8)
             = -torch.log(0.924 + 1e-8)
             = -torch.log(0.924)
             = 0.079

# è®¡ç®—è´Ÿæ ·æœ¬æŸå¤±
negative_loss = -torch.sum(torch.log(torch.sigmoid(-[1.0, 0.5, -0.5, -1.0, -2.0]) + 1e-8), dim=1)
             = -torch.sum(torch.log([0.27, 0.38, 0.62, 0.73, 0.88] + 1e-8), dim=1)
             = -torch.sum([-1.31, -0.97, -0.48, -0.31, -0.13], dim=1)
             = -(-3.20)
             = 3.20

# æ€»æŸå¤±
loss = torch.mean(positive_loss + negative_loss)
     = torch.mean([0.079 + 3.20])
     = 3.279
```

**æŸå¤±çš„å«ä¹‰**ï¼š
- æˆ‘ä»¬å¸Œæœ› `positive_scores` å¤§ï¼ˆæ¥è¿‘ 1 çš„æ¦‚ç‡ï¼‰
- æˆ‘ä»¬å¸Œæœ› `negative_scores` å°ï¼ˆæ¥è¿‘ 0 çš„æ¦‚ç‡ï¼‰
- æŸå¤±è¶Šå°è¶Šå¥½

---

### æ­¥éª¤5ï¼šåå‘ä¼ æ’­ï¼ˆç¬¬ 104 è¡Œï¼‰- `loss.backward()` â­â­â­

**è¿™æ˜¯ PyTorch è‡ªåŠ¨æ±‚å¯¼çš„æ ¸å¿ƒï¼ç†è§£è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼**

#### PyTorch å¦‚ä½•è·Ÿè¸ªå‰å‘ä¼ æ’­ï¼ŸğŸ”

åœ¨æ·±å…¥ç†è§£ `loss.backward()` ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆäº†è§£ **PyTorch å¦‚ä½•åœ¨å‰å‘ä¼ æ’­æ—¶è·Ÿè¸ªæ‰€æœ‰æ“ä½œ**ã€‚

**æ ¸å¿ƒæœºåˆ¶ï¼šè®¡ç®—å›¾ï¼ˆComputation Graphï¼‰**

PyTorch ä½¿ç”¨**åŠ¨æ€è®¡ç®—å›¾**æ¥è·Ÿè¸ªå‰å‘ä¼ æ’­çš„æ¯ä¸€æ­¥æ“ä½œã€‚è®©æˆ‘ä»¬è¯¦ç»†çœ‹çœ‹è¿™ä¸ªè¿‡ç¨‹ï¼š

##### 1. `requires_grad` æ ‡å¿—

**å…³é”®æ¦‚å¿µ**ï¼šåªæœ‰è®¾ç½®äº† `requires_grad=True` çš„å¼ é‡æ‰ä¼šè¢«è·Ÿè¸ªã€‚

```python
# æ¨¡å‹å‚æ•°çš„é»˜è®¤è®¾ç½®
model = Word2Vec(1000, 32)
# center_embed.weight.requires_grad = True  (é»˜è®¤)
# outside_embed.weight.requires_grad = True  (é»˜è®¤)

# è¿™æ„å‘³ç€è¿™äº›å‚æ•°ä¼šè¢«è·Ÿè¸ªï¼Œç”¨äºè®¡ç®—æ¢¯åº¦
```

**ç¤ºä¾‹å¯¹æ¯”**ï¼š
```python
# ä¼šè¢«è·Ÿè¸ªï¼ˆéœ€è¦æ¢¯åº¦ï¼‰
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = x * 2  # y ä¹Ÿä¼šè¢«è·Ÿè¸ª

# ä¸ä¼šè¢«è·Ÿè¸ªï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
x_no_grad = torch.tensor([1.0, 2.0], requires_grad=False)
y_no_grad = x_no_grad * 2  # y_no_grad ä¸ä¼šè¢«è·Ÿè¸ª
```

##### 2. å‰å‘ä¼ æ’­æ—¶çš„è‡ªåŠ¨è·Ÿè¸ª

**æ¯ä¸€æ­¥æ“ä½œéƒ½ä¼šåˆ›å»ºä¸€ä¸ªèŠ‚ç‚¹**ï¼š

```python
# å‡è®¾æˆ‘ä»¬æ‰§è¡Œå‰å‘ä¼ æ’­
center_tensor = torch.tensor([5], dtype=torch.long)  # ç´¢å¼•ï¼Œä¸éœ€è¦æ¢¯åº¦
v_c = model.center_embed(center_tensor)  # éœ€è¦æ¢¯åº¦ï¼

# PyTorch å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
# 1. center_embed æ˜¯ä¸€ä¸ª nn.Embedding å±‚
# 2. center_embed.weight çš„ requires_grad = True
# 3. å½“æ‰§è¡Œ center_embed(center_tensor) æ—¶ï¼š
#    - PyTorch åˆ›å»ºä¸€ä¸ª EmbeddingBackward èŠ‚ç‚¹
#    - è®°å½•ï¼šè¾“å…¥ = center_tensor, æƒé‡ = center_embed.weight
#    - è¾“å‡º v_c çš„ requires_grad = Trueï¼ˆç»§æ‰¿è‡ªæƒé‡ï¼‰
```

**å¯è§†åŒ–è·Ÿè¸ªè¿‡ç¨‹**ï¼š

```python
# æ­¥éª¤1ï¼šEmbedding æŸ¥æ‰¾
v_c = self.center_embed(center_tensor)
# PyTorch è®°å½•ï¼š
#   èŠ‚ç‚¹1: EmbeddingBackward
#   - è¾“å…¥: center_tensor (ç´¢å¼•ï¼Œä¸éœ€è¦æ¢¯åº¦)
#   - å‚æ•°: center_embed.weight (éœ€è¦æ¢¯åº¦ï¼)
#   - è¾“å‡º: v_c (éœ€è¦æ¢¯åº¦ï¼Œå› ä¸ºæ¥è‡ªéœ€è¦æ¢¯åº¦çš„å‚æ•°)
#   - æ¢¯åº¦å‡½æ•°: embedding_backward_fn

# æ­¥éª¤2ï¼šé€å…ƒç´ ç›¸ä¹˜
element_wise = u_o * v_c
# PyTorch è®°å½•ï¼š
#   èŠ‚ç‚¹2: MulBackward
#   - è¾“å…¥1: u_o (éœ€è¦æ¢¯åº¦)
#   - è¾“å…¥2: v_c (éœ€è¦æ¢¯åº¦)
#   - è¾“å‡º: element_wise (éœ€è¦æ¢¯åº¦)
#   - æ¢¯åº¦å‡½æ•°: mul_backward_fn

# æ­¥éª¤3ï¼šæ±‚å’Œ
positive_scores = torch.sum(element_wise, dim=1)
# PyTorch è®°å½•ï¼š
#   èŠ‚ç‚¹3: SumBackward
#   - è¾“å…¥: element_wise (éœ€è¦æ¢¯åº¦)
#   - è¾“å‡º: positive_scores (éœ€è¦æ¢¯åº¦)
#   - æ¢¯åº¦å‡½æ•°: sum_backward_fn

# æ­¥éª¤4ï¼šSigmoid
prob_pos = torch.sigmoid(positive_scores)
# PyTorch è®°å½•ï¼š
#   èŠ‚ç‚¹4: SigmoidBackward
#   - è¾“å…¥: positive_scores (éœ€è¦æ¢¯åº¦)
#   - è¾“å‡º: prob_pos (éœ€è¦æ¢¯åº¦)
#   - æ¢¯åº¦å‡½æ•°: sigmoid_backward_fn

# ... ä»¥æ­¤ç±»æ¨ï¼Œç›´åˆ° loss
```

##### 3. è®¡ç®—å›¾çš„æ„å»º

**æ¯ä¸ªæ“ä½œéƒ½ä¼šåˆ›å»ºä¸€ä¸ª Function å¯¹è±¡**ï¼š

```python
# PyTorch å†…éƒ¨å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
class MulBackward(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input1, input2):
        # å‰å‘ä¼ æ’­ï¼šæ‰§è¡Œä¹˜æ³•
        result = input1 * input2
        # ä¿å­˜ç”¨äºåå‘ä¼ æ’­çš„ä¿¡æ¯
        ctx.save_for_backward(input1, input2)
        return result
    
    @staticmethod
    def backward(ctx, grad_output):
        # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        input1, input2 = ctx.saved_tensors
        grad_input1 = grad_output * input2
        grad_input2 = grad_output * input1
        return grad_input1, grad_input2
```

**å®é™…çš„è®¡ç®—å›¾ç»“æ„**ï¼š

```
è®¡ç®—å›¾ï¼ˆæœ‰å‘æ— ç¯å›¾ DAGï¼‰ï¼š

center_embed.weight (å¶å­èŠ‚ç‚¹ï¼Œéœ€è¦æ¢¯åº¦)
    â”‚
    â”œâ”€â†’ EmbeddingBackward
    â”‚       â”‚
    â”‚       â””â”€â†’ v_c (ä¸­é—´èŠ‚ç‚¹ï¼Œéœ€è¦æ¢¯åº¦)
    â”‚               â”‚
    â”‚               â”œâ”€â†’ MulBackward â† u_o (æ¥è‡ª outside_embed)
    â”‚               â”‚       â”‚
    â”‚               â”‚       â””â”€â†’ element_wise
    â”‚               â”‚               â”‚
    â”‚               â”‚               â””â”€â†’ SumBackward
    â”‚               â”‚                       â”‚
    â”‚               â”‚                       â””â”€â†’ positive_scores
    â”‚               â”‚                               â”‚
    â”‚               â”‚                               â””â”€â†’ SigmoidBackward
    â”‚               â”‚                                       â”‚
    â”‚               â”‚                                       â””â”€â†’ prob_pos
    â”‚               â”‚                                               â”‚
    â”‚               â”‚                                               â””â”€â†’ LogBackward
    â”‚               â”‚                                                       â”‚
    â”‚               â”‚                                                       â””â”€â†’ positive_loss
    â”‚               â”‚                                                               â”‚
    â”‚               â””â”€â†’ BmmBackward (ç”¨äºè´Ÿæ ·æœ¬)                                    â”‚
    â”‚                       â”‚                                                      â”‚
    â”‚                       â””â”€â†’ negative_scores                                    â”‚
    â”‚                               â”‚                                              â”‚
    â”‚                               â””â”€â†’ ... (ç±»ä¼¼è·¯å¾„)                            â”‚
    â”‚                                                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                                    â”‚
                                                                                    â””â”€â†’ AddBackward
                                                                                            â”‚
                                                                                            â””â”€â†’ loss (æ ¹èŠ‚ç‚¹)
```

##### 4. å…³é”®æ•°æ®ç»“æ„

**æ¯ä¸ªå¼ é‡éƒ½åŒ…å«æ¢¯åº¦è·Ÿè¸ªä¿¡æ¯**ï¼š

```python
# æŸ¥çœ‹å¼ é‡çš„æ¢¯åº¦è·Ÿè¸ªä¿¡æ¯
v_c = model.center_embed(center_tensor)

# v_c çš„å†…éƒ¨å±æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
# v_c.data: å®é™…æ•°æ® [0.5, 0.3, ..., 0.2]
# v_c.requires_grad: True
# v_c.grad_fn: EmbeddingBackward å¯¹è±¡ï¼ˆè®°å½•å¦‚ä½•è®¡ç®—æ¢¯åº¦ï¼‰
# v_c.grad: Noneï¼ˆå‰å‘ä¼ æ’­æ—¶è¿˜æ²¡æœ‰æ¢¯åº¦ï¼‰

# grad_fn åŒ…å«çš„ä¿¡æ¯ï¼š
# - æ“ä½œç±»å‹ï¼šEmbedding
# - è¾“å…¥èŠ‚ç‚¹ï¼šcenter_tensor
# - å‚æ•°èŠ‚ç‚¹ï¼šcenter_embed.weight
# - æ¢¯åº¦å‡½æ•°ï¼šå¦‚ä½•è®¡ç®—æ¢¯åº¦
```

##### 5. å‰å‘ä¼ æ’­è·Ÿè¸ªçš„å®Œæ•´ç¤ºä¾‹

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•ä¾‹å­çœ‹å®Œæ•´è¿‡ç¨‹ï¼š

```python
import torch

# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å‚æ•°
x = torch.tensor([2.0], requires_grad=True)  # å¶å­èŠ‚ç‚¹
y = torch.tensor([3.0], requires_grad=True)  # å¶å­èŠ‚ç‚¹

# å‰å‘ä¼ æ’­ï¼ˆæ¯ä¸€æ­¥éƒ½è¢«è·Ÿè¸ªï¼‰
z = x * y        # èŠ‚ç‚¹1: MulBackward
w = z + 1        # èŠ‚ç‚¹2: AddBackward
loss = w ** 2    # èŠ‚ç‚¹3: PowBackward

# æ­¤æ—¶çš„è®¡ç®—å›¾ï¼š
# x (å¶å­) â”€â”€â†’ MulBackward â”€â”€â†’ z â”€â”€â†’ AddBackward â”€â”€â†’ w â”€â”€â†’ PowBackward â”€â”€â†’ loss
# y (å¶å­) â”€â”€â”˜

# æŸ¥çœ‹è·Ÿè¸ªä¿¡æ¯
print(loss.grad_fn)           # <PowBackward0 object>
print(loss.grad_fn.next_functions)  # ((<AddBackward0 object>, 0),)
print(w.grad_fn)              # <AddBackward0 object>
print(z.grad_fn)              # <MulBackward0 object>
```

##### 6. Word2Vec ä¸­çš„å®é™…è·Ÿè¸ª

**åœ¨æˆ‘ä»¬çš„ä»£ç ä¸­**ï¼š

```python
# æ­¥éª¤3ï¼šå‰å‘ä¼ æ’­
positive_scores, negative_scores = model(center_tensor, context_tensor, neg_tensor)

# å†…éƒ¨å‘ç”Ÿçš„è·Ÿè¸ªï¼š
# 1. center_embed(center_tensor)
#    â†’ åˆ›å»º EmbeddingBackward èŠ‚ç‚¹
#    â†’ v_c.grad_fn = EmbeddingBackward(center_embed.weight)
#
# 2. u_o * v_c
#    â†’ åˆ›å»º MulBackward èŠ‚ç‚¹
#    â†’ element_wise.grad_fn = MulBackward(u_o, v_c)
#
# 3. torch.sum(element_wise, dim=1)
#    â†’ åˆ›å»º SumBackward èŠ‚ç‚¹
#    â†’ positive_scores.grad_fn = SumBackward(element_wise)
#
# 4. torch.sigmoid(positive_scores)
#    â†’ åˆ›å»º SigmoidBackward èŠ‚ç‚¹
#    â†’ prob_pos.grad_fn = SigmoidBackward(positive_scores)
#
# ... ç›´åˆ° loss
#    â†’ loss.grad_fn = MeanBackward(...)
```

**å…³é”®ç‚¹**ï¼š
- **åªæœ‰éœ€è¦æ¢¯åº¦çš„å¼ é‡æ‰ä¼šè¢«è·Ÿè¸ª**
- **æ¯ä¸ªæ“ä½œåˆ›å»ºä¸€ä¸ªèŠ‚ç‚¹ï¼Œè®°å½•æ“ä½œç±»å‹å’Œè¾“å…¥**
- **èŠ‚ç‚¹ä¹‹é—´é€šè¿‡ `grad_fn` è¿æ¥ï¼Œå½¢æˆè®¡ç®—å›¾**
- **è®¡ç®—å›¾æ˜¯åŠ¨æ€çš„**ï¼šæ¯æ¬¡å‰å‘ä¼ æ’­éƒ½ä¼šé‡æ–°æ„å»º

##### 7. ä¸ºä»€ä¹ˆéœ€è¦è·Ÿè¸ªï¼Ÿ

**è·Ÿè¸ªçš„ç›®çš„**ï¼š
1. **è®°å½•æ“ä½œå†å²**ï¼šçŸ¥é“æ¯ä¸ªå€¼æ˜¯å¦‚ä½•è®¡ç®—çš„
2. **å‡†å¤‡åå‘ä¼ æ’­**ï¼šçŸ¥é“å¦‚ä½•è®¡ç®—æ¢¯åº¦
3. **è‡ªåŠ¨æ±‚å¯¼**ï¼šæ— éœ€æ‰‹åŠ¨æ¨å¯¼æ¢¯åº¦å…¬å¼

**ä¸è·Ÿè¸ªçš„æƒ…å†µ**ï¼š
```python
# å¦‚æœä¸éœ€è¦æ¢¯åº¦ï¼Œå¯ä»¥å…³é—­è·Ÿè¸ªï¼ˆèŠ‚çœå†…å­˜å’Œè®¡ç®—ï¼‰
with torch.no_grad():
    # è¿™é‡Œçš„æ“ä½œä¸ä¼šè¢«è·Ÿè¸ª
    result = model(input)  # ä¸ä¼šæ„å»ºè®¡ç®—å›¾
```

---

#### `loss.backward()` å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

**PyTorch çš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶ï¼ˆAutogradï¼‰**ï¼š

1. **æ„å»ºè®¡ç®—å›¾**ï¼ˆå·²å®Œæˆï¼‰ï¼š
   - åœ¨å‰å‘ä¼ æ’­æ—¶ï¼ŒPyTorch è‡ªåŠ¨è®°å½•æ‰€æœ‰æ“ä½œ
   - æ„å»ºä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œè®°å½•æ•°æ®æµå‘
   - æ¯ä¸ªèŠ‚ç‚¹å­˜å‚¨äº†å¦‚ä½•è®¡ç®—æ¢¯åº¦çš„å‡½æ•°

2. **åå‘ä¼ æ’­**ï¼š
   - ä»æŸå¤±å€¼å¼€å§‹ï¼Œæ²¿ç€è®¡ç®—å›¾åå‘ä¼ æ’­
   - ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦
   - è°ƒç”¨æ¯ä¸ªèŠ‚ç‚¹çš„ `grad_fn.backward()` æ–¹æ³•

#### è¯¦ç»†çš„è®¡ç®—å›¾ç¤ºä¾‹

è®©æˆ‘ä»¬è¿½è¸ªä¸€ä¸ªå®Œæ•´çš„è®¡ç®—å›¾ï¼š

```python
# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ ·æœ¬ï¼š
center_tensor = [5]
context_tensor = [10]
neg_tensor = [[20, 30]]

# å‰å‘ä¼ æ’­æ„å»ºçš„è®¡ç®—å›¾ï¼š
# 
# è¾“å…¥å±‚
#   center_tensor[5] â”€â”€â”
#   context_tensor[10] â”€â”€â”¤
#   neg_tensor[[20,30]] â”€â”˜
#                        â”‚
# Embedding å±‚
#   center_embed.weight[5] â”€â”€â†’ v_c (1, 32)
#   outside_embed.weight[10] â”€â”€â†’ u_o (1, 32)
#   outside_embed.weight[20] â”€â”€â†’ u_neg[0] (32,)
#   outside_embed.weight[30] â”€â”€â†’ u_neg[1] (32,)
#                        â”‚
# è®¡ç®—å±‚
#   u_o * v_c â”€â”€â†’ element_wise (1, 32)
#   sum(element_wise, dim=1) â”€â”€â†’ positive_scores (1,)
#   v_c Â· u_neg[0] â”€â”€â†’ neg_score_0 (1,)
#   v_c Â· u_neg[1] â”€â”€â†’ neg_score_1 (1,)
#                        â”‚
# æŸå¤±å±‚
#   sigmoid(positive_scores) â”€â”€â†’ prob_pos (1,)
#   log(prob_pos) â”€â”€â†’ log_prob_pos (1,)
#   -log_prob_pos â”€â”€â†’ positive_loss (1,)
#   sigmoid(-neg_score_0) â”€â”€â†’ prob_neg_0 (1,)
#   log(prob_neg_0) â”€â”€â†’ log_prob_neg_0 (1,)
#   ... (å¯¹ neg_score_1 åŒæ ·æ“ä½œ)
#   sum(negative_losses) â”€â”€â†’ negative_loss (1,)
#   positive_loss + negative_loss â”€â”€â†’ total_loss (1,)
#   mean(total_loss) â”€â”€â†’ loss (æ ‡é‡)
```

#### åå‘ä¼ æ’­çš„æ•°å­¦è¿‡ç¨‹

**é“¾å¼æ³•åˆ™**ï¼š`d(loss)/d(param) = d(loss)/d(output) Ã— d(output)/d(param)`

**å…·ä½“è®¡ç®—**ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š

```python
# å‡è®¾ loss = 3.279

# 1. è®¡ç®— loss å¯¹ positive_scores çš„æ¢¯åº¦
d_loss/d_positive_scores = d/d_positive_scores[-log(sigmoid(positive_scores))]
                         = -1/sigmoid(positive_scores) Ã— sigmoid'(positive_scores)
                         = -1/0.924 Ã— 0.924Ã—(1-0.924)
                         = -0.076

# 2. è®¡ç®— loss å¯¹ negative_scores çš„æ¢¯åº¦ï¼ˆå¯¹æ¯ä¸ªè´Ÿæ ·æœ¬ï¼‰
d_loss/d_negative_scores[0] = ... (ç±»ä¼¼è®¡ç®—)
d_loss/d_negative_scores[1] = ...

# 3. è®¡ç®— loss å¯¹ v_c çš„æ¢¯åº¦
d_loss/d_v_c = d_loss/d_positive_scores Ã— d_positive_scores/d_v_c
             + d_loss/d_negative_scores Ã— d_negative_scores/d_v_c
             = -0.076 Ã— u_o + (è´Ÿæ ·æœ¬æ¢¯åº¦é¡¹)

# 4. è®¡ç®— loss å¯¹ u_o çš„æ¢¯åº¦
d_loss/d_u_o = d_loss/d_positive_scores Ã— d_positive_scores/d_u_o
             = -0.076 Ã— v_c

# 5. è®¡ç®— loss å¯¹ u_neg çš„æ¢¯åº¦ï¼ˆå¯¹æ¯ä¸ªè´Ÿæ ·æœ¬ï¼‰
d_loss/d_u_neg[0] = d_loss/d_negative_scores[0] Ã— d_negative_scores[0]/d_u_neg[0]
                  = (æŸä¸ªå€¼) Ã— v_c

# 6. æœ€ç»ˆï¼Œè®¡ç®— loss å¯¹ Embedding çŸ©é˜µçš„æ¢¯åº¦
# center_embed.weight[5] çš„æ¢¯åº¦ = d_loss/d_v_c
# outside_embed.weight[10] çš„æ¢¯åº¦ = d_loss/d_u_o
# outside_embed.weight[20] çš„æ¢¯åº¦ = d_loss/d_u_neg[0]
# outside_embed.weight[30] çš„æ¢¯åº¦ = d_loss/d_u_neg[1]
```

#### PyTorch å†…éƒ¨å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
# loss.backward() çš„å†…éƒ¨é€»è¾‘ï¼ˆä¼ªä»£ç ï¼‰
def backward(self, loss):
    # 1. ä»æŸå¤±å€¼å¼€å§‹
    loss.grad = 1.0  # æŸå¤±å¯¹è‡ªèº«çš„æ¢¯åº¦æ˜¯ 1
    
    # 2. åå‘éå†è®¡ç®—å›¾
    for node in reversed(computation_graph):
        # 3. è®¡ç®—å½“å‰èŠ‚ç‚¹çš„æ¢¯åº¦
        node.grad = compute_gradient(node, loss.grad)
        
        # 4. ä¼ é€’ç»™å‰ä¸€ä¸ªèŠ‚ç‚¹
        for prev_node in node.inputs:
            prev_node.grad += node.grad Ã— node.gradient_wrt(prev_node)
    
    # 5. å°†æ¢¯åº¦å­˜å‚¨åˆ°å‚æ•°çš„ .grad å±æ€§
    for param in model.parameters():
        param.grad = param.node.grad
```

#### å®é™…å­˜å‚¨çš„æ¢¯åº¦

```python
# æ‰§è¡Œ loss.backward() åï¼š

# center_embed.weight.grad å½¢çŠ¶ï¼š(1000, 32)
# åªæœ‰ç¬¬ 5 è¡Œæœ‰éé›¶æ¢¯åº¦ï¼ˆå› ä¸ºæˆ‘ä»¬åªç”¨äº†ç´¢å¼• 5ï¼‰
center_embed.weight.grad[5] = [0.05, -0.02, 0.01, ..., -0.03]  # 32 ä¸ªæ¢¯åº¦å€¼
center_embed.weight.grad[å…¶ä»–è¡Œ] = [0, 0, ..., 0]  # å…¶ä»–è¡Œæ¢¯åº¦ä¸º 0

# outside_embed.weight.grad å½¢çŠ¶ï¼š(1000, 32)
# ç¬¬ 10ã€20ã€30 è¡Œæœ‰éé›¶æ¢¯åº¦
outside_embed.weight.grad[10] = [-0.03, 0.01, -0.02, ..., 0.01]
outside_embed.weight.grad[20] = [0.02, -0.01, 0.01, ..., -0.02]
outside_embed.weight.grad[30] = [0.01, 0.02, -0.01, ..., 0.01]
outside_embed.weight.grad[å…¶ä»–è¡Œ] = [0, 0, ..., 0]
```

**å…³é”®ç†è§£**ï¼š
- **åªæœ‰è¢«ä½¿ç”¨çš„å‚æ•°è¡Œæ‰ä¼šæœ‰æ¢¯åº¦**
- ä¾‹å¦‚ï¼Œ`center_embed.weight[5]` æœ‰æ¢¯åº¦ï¼Œä½† `center_embed.weight[100]` æ²¡æœ‰ï¼ˆå› ä¸ºè¿™æ¬¡æ²¡ç”¨ï¼‰
- æ¢¯åº¦å‘Šè¯‰æˆ‘ä»¬ï¼š**å¦‚ä½•è°ƒæ•´å‚æ•°æ¥å‡å°æŸå¤±**

---

### æ­¥éª¤6ï¼šæ›´æ–°å‚æ•°ï¼ˆç¬¬ 105 è¡Œï¼‰- `optimizer.step()` â­â­â­

**è¿™æ˜¯å‚æ•°æ›´æ–°çš„æœ€åä¸€æ­¥ï¼**

#### `optimizer.step()` å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

**AdamW ä¼˜åŒ–å™¨çš„æ›´æ–°è¿‡ç¨‹**ï¼š

```python
# optimizer.step() çš„å†…éƒ¨é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
def step(self):
    for param in self.parameters():
        if param.grad is None:
            continue
        
        # 1. è¯»å–æ¢¯åº¦
        grad = param.grad  # å½¢çŠ¶ï¼š(1000, 32)
        
        # 2. AdamW ç®—æ³•è®¡ç®—æ›´æ–°é‡
        # ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™… AdamW æ›´å¤æ‚ï¼Œæ¶‰åŠåŠ¨é‡å’Œè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
        update = learning_rate * grad
        
        # 3. æ›´æ–°å‚æ•°
        param.data = param.data - update
```

#### å…·ä½“æ›´æ–°ç¤ºä¾‹

**æ›´æ–°å‰**ï¼š
```python
# center_embed.weight[5] æ›´æ–°å‰
center_embed.weight.data[5] = [0.5, 0.3, 0.1, ..., 0.2]

# æ¢¯åº¦
center_embed.weight.grad[5] = [0.05, -0.02, 0.01, ..., -0.03]

# å­¦ä¹ ç‡
learning_rate = 0.01
```

**æ›´æ–°è¿‡ç¨‹**ï¼š
```python
# è®¡ç®—æ›´æ–°é‡
update = learning_rate * grad
       = 0.01 * [0.05, -0.02, 0.01, ..., -0.03]
       = [0.0005, -0.0002, 0.0001, ..., -0.0003]

# æ›´æ–°å‚æ•°
center_embed.weight.data[5] = center_embed.weight.data[5] - update
                            = [0.5, 0.3, 0.1, ..., 0.2] - [0.0005, -0.0002, 0.0001, ..., -0.0003]
                            = [0.4995, 0.3002, 0.0999, ..., 0.2003]
```

**æ›´æ–°å**ï¼š
```python
# center_embed.weight[5] æ›´æ–°å
center_embed.weight.data[5] = [0.4995, 0.3002, 0.0999, ..., 0.2003]
```

#### AdamW ä¼˜åŒ–å™¨çš„ä¼˜åŠ¿

**AdamW vs ç®€å• SGD**ï¼š

```python
# ç®€å• SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰
param = param - learning_rate * grad

# AdamWï¼ˆè‡ªé€‚åº”å­¦ä¹ ç‡ï¼‰
# 1. ç»´æŠ¤æ¯ä¸ªå‚æ•°çš„å†å²æ¢¯åº¦ï¼ˆåŠ¨é‡ï¼‰
# 2. è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡ï¼ˆæ¯ä¸ªå‚æ•°ä¸åŒï¼‰
# 3. æƒé‡è¡°å‡ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
# æ›´æ–°å…¬å¼æ›´å¤æ‚ï¼Œä½†è®­ç»ƒæ›´ç¨³å®šã€æ›´å¿«
```

**AdamW çš„ä¼˜åŠ¿**ï¼š
- **è‡ªé€‚åº”å­¦ä¹ ç‡**ï¼šæ¯ä¸ªå‚æ•°æœ‰è‡ªå·±çš„å­¦ä¹ ç‡
- **åŠ¨é‡**ï¼šåˆ©ç”¨å†å²æ¢¯åº¦ä¿¡æ¯ï¼Œè®­ç»ƒæ›´ç¨³å®š
- **æƒé‡è¡°å‡**ï¼šé˜²æ­¢å‚æ•°è¿‡å¤§ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

#### ä¸¤ä¸ª Embedding å±‚å¦‚ä½•åŒæ—¶æ›´æ–°ï¼Ÿ

**å…³é”®**ï¼šä¸¤ä¸ª Embedding å±‚çš„å‚æ•°**ç‹¬ç«‹æ›´æ–°**ï¼Œä½†**ååŒä¼˜åŒ–**ã€‚

```python
# ä¸€æ¬¡è¿­ä»£ä¸­ï¼š

# 1. center_embed çš„æ›´æ–°
# åªæ›´æ–°è¢«ä½¿ç”¨çš„è¡Œï¼ˆå¦‚ç¬¬ 5 è¡Œï¼‰
center_embed.weight.data[5] -= learning_rate * center_embed.weight.grad[5]

# 2. outside_embed çš„æ›´æ–°
# æ›´æ–°è¢«ä½¿ç”¨çš„è¡Œï¼ˆå¦‚ç¬¬ 10ã€20ã€30 è¡Œï¼‰
outside_embed.weight.data[10] -= learning_rate * outside_embed.weight.grad[10]
outside_embed.weight.data[20] -= learning_rate * outside_embed.weight.grad[20]
outside_embed.weight.data[30] -= learning_rate * outside_embed.weight.grad[30]
```

**ååŒå·¥ä½œçš„ä½“ç°**ï¼š
- ä¸¤ä¸ªçŸ©é˜µçš„**åŒä¸€è¡Œå¯¹åº”åŒä¸€ä¸ªè¯**
- ä½†æ›´æ–°æ˜¯**ç‹¬ç«‹çš„**ï¼Œå› ä¸ºå®ƒä»¬åœ¨ä¸åŒçš„è§’è‰²ï¼ˆä¸­å¿ƒè¯ vs ä¸Šä¸‹æ–‡è¯ï¼‰
- é€šè¿‡æŸå¤±å‡½æ•°ï¼Œä¸¤ä¸ªçŸ©é˜µ**å…±åŒä¼˜åŒ–**ï¼Œè®©ç›¸ä¼¼è¯çš„å‘é‡æ¥è¿‘

**è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–**ï¼š
```python
# è®­ç»ƒå‰ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰
center_embed.weight[5] = [0.5, 0.3, 0.1, ..., 0.2]  # "cat"
outside_embed.weight[10] = [0.4, 0.2, 0.3, ..., 0.1]  # "dog"
# ä¸¤ä¸ªå‘é‡å¯èƒ½ä¸ç›¸ä¼¼

# è®­ç»ƒåï¼ˆç»è¿‡å¤šæ¬¡æ›´æ–°ï¼‰
center_embed.weight[5] = [0.52, 0.28, 0.12, ..., 0.18]  # "cat"
outside_embed.weight[10] = [0.51, 0.29, 0.11, ..., 0.19]  # "dog"
# ä¸¤ä¸ªå‘é‡å˜å¾—æ›´ç›¸ä¼¼ï¼ˆå› ä¸º "cat" å’Œ "dog" ç»å¸¸å…±ç°ï¼‰
```

---

### ä¼˜åŒ–å™¨åˆå§‹åŒ–è¯¦è§£ï¼š`optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)` ğŸ”

è¿™æ˜¯è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®ä¸€æ­¥ã€‚è®©æˆ‘ä»¬æ·±å…¥ç†è§£è¿™å¥è¯çš„å«ä¹‰ã€‚

#### `model.parameters()` è¿”å›ä»€ä¹ˆï¼Ÿ

**`model.parameters()` æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨**ï¼Œè¿”å›æ¨¡å‹ä¸­æ‰€æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°ã€‚

**åœ¨æˆ‘ä»¬çš„ Word2Vec æ¨¡å‹ä¸­**ï¼š

```python
model = Word2Vec(vocab_size=1000, embedding_dim=32)

# model.parameters() è¿”å›ä»€ä¹ˆï¼Ÿ
for param in model.parameters():
    print(param.shape, param.requires_grad)

# è¾“å‡ºï¼š
# torch.Size([1000, 32]) True  â† center_embed.weight
# torch.Size([1000, 32]) True  â† outside_embed.weight
```

**å…·ä½“å†…å®¹**ï¼š

```python
# æ¨¡å‹ç»“æ„
class Word2Vec(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(Word2Vec, self).__init__()
        self.center_embed = nn.Embedding(vocab_size, embedding_dim)  # ç¬¬1å±‚
        self.outside_embed = nn.Embedding(vocab_size, embedding_dim)  # ç¬¬2å±‚

# model.parameters() è¿”å›çš„å‚æ•°ï¼š
# 1. center_embed.weight: (1000, 32) - 32,000 ä¸ªå‚æ•°
# 2. outside_embed.weight: (1000, 32) - 32,000 ä¸ªå‚æ•°
# æ€»è®¡ï¼š64,000 ä¸ªå¯è®­ç»ƒå‚æ•°
```

**éªŒè¯å‚æ•°**ï¼š

```python
# æŸ¥çœ‹æ‰€æœ‰å‚æ•°
params = list(model.parameters())
print(f"å‚æ•°æ€»æ•°: {len(params)}")  # 2 ä¸ªå‚æ•°ç»„

# æŸ¥çœ‹æ¯ä¸ªå‚æ•°çš„è¯¦ç»†ä¿¡æ¯
for i, param in enumerate(params):
    print(f"å‚æ•° {i+1}:")
    print(f"  å½¢çŠ¶: {param.shape}")
    print(f"  å‚æ•°æ•°é‡: {param.numel()}")  # numel = number of elements
    print(f"  éœ€è¦æ¢¯åº¦: {param.requires_grad}")

# è¾“å‡ºï¼š
# å‚æ•°æ€»æ•°: 2
# å‚æ•° 1:
#   å½¢çŠ¶: torch.Size([1000, 32])
#   å‚æ•°æ•°é‡: 32000
#   éœ€è¦æ¢¯åº¦: True
# å‚æ•° 2:
#   å½¢çŠ¶: torch.Size([1000, 32])
#   å‚æ•°æ•°é‡: 32000
#   éœ€è¦æ¢¯åº¦: True
```

#### å½“å‰æ¨¡å‹æœ‰å¤šå°‘å±‚ï¼Ÿ

**Word2Vec æ¨¡å‹åªæœ‰ 2 å±‚**ï¼š

1. **`center_embed`**ï¼š`nn.Embedding` å±‚
   - å‚æ•°ï¼š`center_embed.weight`ï¼Œå½¢çŠ¶ `(vocab_size, embedding_dim)`
   - ä½œç”¨ï¼šå°†ä¸­å¿ƒè¯ç´¢å¼•æ˜ å°„ä¸ºå‘é‡

2. **`outside_embed`**ï¼š`nn.Embedding` å±‚
   - å‚æ•°ï¼š`outside_embed.weight`ï¼Œå½¢çŠ¶ `(vocab_size, embedding_dim)`
   - ä½œç”¨ï¼šå°†ä¸Šä¸‹æ–‡è¯ç´¢å¼•æ˜ å°„ä¸ºå‘é‡

**ä¸ºä»€ä¹ˆåªæœ‰ 2 å±‚ï¼Ÿ**

Word2Vec æ˜¯ä¸€ä¸ª**ç®€å•çš„æ¨¡å‹**ï¼Œä¸éœ€è¦å¤æ‚çš„ç¥ç»ç½‘ç»œç»“æ„ï¼š

- **ä¸éœ€è¦å…¨è¿æ¥å±‚ï¼ˆLinearï¼‰**ï¼šç›´æ¥ä½¿ç”¨ç‚¹ç§¯è®¡ç®—ç›¸ä¼¼åº¦
- **ä¸éœ€è¦æ¿€æ´»å‡½æ•°å±‚**ï¼šç›¸ä¼¼åº¦è®¡ç®—åœ¨æŸå¤±å‡½æ•°ä¸­å¤„ç†
- **ä¸éœ€è¦å·ç§¯å±‚ï¼ˆConvï¼‰**ï¼šå¤„ç†çš„æ˜¯ç¦»æ•£çš„è¯ç´¢å¼•ï¼Œä¸æ˜¯å›¾åƒ
- **ä¸éœ€è¦å¾ªç¯å±‚ï¼ˆRNN/LSTMï¼‰**ï¼šSkip-Gram æ¨¡å‹åªçœ‹å±€éƒ¨ä¸Šä¸‹æ–‡

**æ¨¡å‹ç»“æ„å¯¹æ¯”**ï¼š

```python
# Word2Vecï¼ˆç®€å•ï¼‰
class Word2Vec(nn.Module):
    def __init__(self):
        self.center_embed = nn.Embedding(...)   # å±‚1
        self.outside_embed = nn.Embedding(...)   # å±‚2
    # åªæœ‰ 2 å±‚ï¼Œæ²¡æœ‰å…¶ä»–å±‚

# å¤æ‚çš„æ¨¡å‹ï¼ˆå¯¹æ¯”ï¼‰
class ComplexModel(nn.Module):
    def __init__(self):
        self.embedding = nn.Embedding(...)       # å±‚1
        self.linear1 = nn.Linear(...)            # å±‚2
        self.relu = nn.ReLU()                    # å±‚3ï¼ˆæ¿€æ´»å‡½æ•°ï¼‰
        self.linear2 = nn.Linear(...)            # å±‚4
        self.dropout = nn.Dropout(...)           # å±‚5
    # æœ‰ 5 å±‚
```

#### ä¸ºä»€ä¹ˆä¸éœ€è¦åˆ†å±‚ï¼Ÿ

**å…³é”®ç†è§£**ï¼š`model.parameters()` **å·²ç»è‡ªåŠ¨æ”¶é›†äº†æ‰€æœ‰å±‚çš„å‚æ•°**ï¼Œä¸éœ€è¦æ‰‹åŠ¨åˆ†å±‚ã€‚

**PyTorch çš„è‡ªåŠ¨å‚æ•°æ”¶é›†æœºåˆ¶**ï¼š

```python
# nn.Module çš„ __init__ ä¼šè‡ªåŠ¨æ³¨å†Œå‚æ•°
class Word2Vec(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(Word2Vec, self).__init__()
        # è¿™ä¸¤è¡Œä¼šè‡ªåŠ¨æ³¨å†Œå‚æ•°åˆ°æ¨¡å‹
        self.center_embed = nn.Embedding(vocab_size, embedding_dim)
        self.outside_embed = nn.Embedding(vocab_size, embedding_dim)
        # PyTorch å†…éƒ¨ï¼š
        # self._parameters['center_embed.weight'] = center_embed.weight
        # self._parameters['outside_embed.weight'] = outside_embed.weight

# model.parameters() ä¼šè‡ªåŠ¨è¿”å›æ‰€æœ‰æ³¨å†Œçš„å‚æ•°
# ä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®šæ¯ä¸€å±‚
```

**å¦‚æœæ‰‹åŠ¨åˆ†å±‚ä¼šæ€æ ·ï¼Ÿ**

```python
# æ–¹å¼1ï¼šå…¨éƒ¨å‚æ•°ï¼ˆæ¨èï¼Œç®€å•ï¼‰
optimizer = optim.AdamW(model.parameters(), lr=0.01)
# è‡ªåŠ¨æ”¶é›†æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬ center_embed å’Œ outside_embed

# æ–¹å¼2ï¼šæ‰‹åŠ¨åˆ†å±‚ï¼ˆä¸å¿…è¦ï¼Œä½†å¯ä»¥ï¼‰
optimizer = optim.AdamW([
    {'params': model.center_embed.parameters()},   # ç¬¬1å±‚çš„å‚æ•°
    {'params': model.outside_embed.parameters()}    # ç¬¬2å±‚çš„å‚æ•°
], lr=0.01)
# æ•ˆæœç›¸åŒï¼Œä½†ä»£ç æ›´å¤æ‚

# æ–¹å¼3ï¼šä¸åŒå±‚ä¸åŒå­¦ä¹ ç‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
optimizer = optim.AdamW([
    {'params': model.center_embed.parameters(), 'lr': 0.01},   # ç¬¬1å±‚å­¦ä¹ ç‡ 0.01
    {'params': model.outside_embed.parameters(), 'lr': 0.005}  # ç¬¬2å±‚å­¦ä¹ ç‡ 0.005
])
# åªæœ‰åœ¨éœ€è¦ä¸åŒå­¦ä¹ ç‡æ—¶æ‰éœ€è¦åˆ†å±‚
```

**ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„ä»£ç ä¸éœ€è¦åˆ†å±‚ï¼Ÿ**

1. **æ¨¡å‹ç®€å•**ï¼šåªæœ‰ 2 å±‚ï¼Œç»“æ„ç›¸åŒ
2. **å­¦ä¹ ç‡ç›¸åŒ**ï¼šä¸¤å±‚ä½¿ç”¨ç›¸åŒçš„å­¦ä¹ ç‡å³å¯
3. **PyTorch è‡ªåŠ¨å¤„ç†**ï¼š`model.parameters()` å·²ç»æ”¶é›†äº†æ‰€æœ‰å‚æ•°

**ä»€ä¹ˆæ—¶å€™éœ€è¦åˆ†å±‚ï¼Ÿ**

```python
# ç¤ºä¾‹ï¼šéœ€è¦ä¸åŒå­¦ä¹ ç‡çš„å¤æ‚æ¨¡å‹
class ComplexModel(nn.Module):
    def __init__(self):
        self.backbone = ResNet50()      # é¢„è®­ç»ƒæ¨¡å‹ï¼Œå­¦ä¹ ç‡å°
        self.classifier = nn.Linear(...) # æ–°å±‚ï¼Œå­¦ä¹ ç‡å¤§

# éœ€è¦åˆ†å±‚ï¼Œå› ä¸ºä¸åŒéƒ¨åˆ†éœ€è¦ä¸åŒå­¦ä¹ ç‡
optimizer = optim.AdamW([
    {'params': model.backbone.parameters(), 'lr': 0.0001},  # é¢„è®­ç»ƒéƒ¨åˆ†ï¼Œå°å­¦ä¹ ç‡
    {'params': model.classifier.parameters(), 'lr': 0.01}   # æ–°éƒ¨åˆ†ï¼Œå¤§å­¦ä¹ ç‡
])
```

#### `optim.AdamW` çš„ä½œç”¨

**AdamW ä¼˜åŒ–å™¨ä¼šä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤**ï¼š

1. **æ¢¯åº¦**ï¼š`param.grad`ï¼ˆç”± `loss.backward()` è®¡ç®—ï¼‰
2. **ä¸€é˜¶åŠ¨é‡**ï¼šå†å²æ¢¯åº¦çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
3. **äºŒé˜¶åŠ¨é‡**ï¼šå†å²æ¢¯åº¦å¹³æ–¹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡
4. **å­¦ä¹ ç‡**ï¼šè‡ªé€‚åº”è°ƒæ•´

**åˆå§‹åŒ–è¿‡ç¨‹**ï¼š

```python
optimizer = optim.AdamW(model.parameters(), lr=0.01)

# å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ
# 1. éå† model.parameters() è¿”å›çš„æ‰€æœ‰å‚æ•°
# 2. ä¸ºæ¯ä¸ªå‚æ•°åˆ›å»ºä¼˜åŒ–å™¨çŠ¶æ€ï¼š
#    - ä¸€é˜¶åŠ¨é‡ï¼šåˆå§‹åŒ–ä¸º 0
#    - äºŒé˜¶åŠ¨é‡ï¼šåˆå§‹åŒ–ä¸º 0
#    - å­¦ä¹ ç‡ï¼šä½¿ç”¨ä¼ å…¥çš„ lr=0.01
# 3. å­˜å‚¨è¿™äº›çŠ¶æ€ï¼Œç”¨äºåç»­çš„å‚æ•°æ›´æ–°

# ä¼˜åŒ–å™¨å†…éƒ¨çŠ¶æ€ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
# optimizer.state = {
#     param1 (center_embed.weight): {
#         'step': 0,
#         'exp_avg': zeros(1000, 32),      # ä¸€é˜¶åŠ¨é‡
#         'exp_avg_sq': zeros(1000, 32),   # äºŒé˜¶åŠ¨é‡
#     },
#     param2 (outside_embed.weight): {
#         'step': 0,
#         'exp_avg': zeros(1000, 32),
#         'exp_avg_sq': zeros(1000, 32),
#     }
# }
```

#### å®Œæ•´ç¤ºä¾‹ï¼šæŸ¥çœ‹æ¨¡å‹å‚æ•°

```python
# åˆ›å»ºæ¨¡å‹
model = Word2Vec(vocab_size=1000, embedding_dim=32)

# æŸ¥çœ‹æ¨¡å‹ç»“æ„
print(model)
# è¾“å‡ºï¼š
# Word2Vec(
#   (center_embed): Embedding(1000, 32)
#   (outside_embed): Embedding(1000, 32)
# )

# æŸ¥çœ‹æ‰€æœ‰å‚æ•°
print("=" * 50)
print("æ¨¡å‹å‚æ•°è¯¦æƒ…ï¼š")
print("=" * 50)
total_params = 0
for name, param in model.named_parameters():
    print(f"å‚æ•°å: {name}")
    print(f"  å½¢çŠ¶: {param.shape}")
    print(f"  å‚æ•°æ•°é‡: {param.numel():,}")
    print(f"  éœ€è¦æ¢¯åº¦: {param.requires_grad}")
    total_params += param.numel()
    print()

print(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")

# è¾“å‡ºï¼š
# ==================================================
# æ¨¡å‹å‚æ•°è¯¦æƒ…ï¼š
# ==================================================
# å‚æ•°å: center_embed.weight
#   å½¢çŠ¶: torch.Size([1000, 32])
#   å‚æ•°æ•°é‡: 32,000
#   éœ€è¦æ¢¯åº¦: True
#
# å‚æ•°å: outside_embed.weight
#   å½¢çŠ¶: torch.Size([1000, 32])
#   å‚æ•°æ•°é‡: 32,000
#   éœ€è¦æ¢¯åº¦: True
#
# æ€»å‚æ•°æ•°é‡: 64,000

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = optim.AdamW(model.parameters(), lr=0.01)

# æŸ¥çœ‹ä¼˜åŒ–å™¨ç®¡ç†çš„å‚æ•°æ•°é‡
print(f"ä¼˜åŒ–å™¨ç®¡ç†çš„å‚æ•°ç»„æ•°: {len(optimizer.param_groups)}")  # 1
print(f"ä¼˜åŒ–å™¨ç®¡ç†çš„å‚æ•°æ€»æ•°: {sum(p.numel() for p in optimizer.param_groups[0]['params'])}")  # 64,000
```

#### å…³é”®è¦ç‚¹æ€»ç»“

1. **`model.parameters()`**ï¼š
   - è¿”å›æ¨¡å‹ä¸­æ‰€æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°ï¼ˆç”Ÿæˆå™¨ï¼‰
   - åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸­è¿”å› 2 ä¸ªå‚æ•°ï¼š`center_embed.weight` å’Œ `outside_embed.weight`
   - æ¯ä¸ªå‚æ•°å½¢çŠ¶ä¸º `(vocab_size, embedding_dim)`

2. **æ¨¡å‹å±‚æ•°**ï¼š
   - Word2Vec åªæœ‰ **2 å±‚**ï¼šä¸¤ä¸ª `nn.Embedding` å±‚
   - æ²¡æœ‰å…¶ä»–å±‚ï¼ˆå¦‚ Linearã€Convã€RNN ç­‰ï¼‰

3. **ä¸ºä»€ä¹ˆä¸åˆ†å±‚**ï¼š
   - `model.parameters()` å·²ç»è‡ªåŠ¨æ”¶é›†äº†æ‰€æœ‰å‚æ•°
   - ä¸¤å±‚ç»“æ„ç›¸åŒï¼Œä½¿ç”¨ç›¸åŒå­¦ä¹ ç‡å³å¯
   - åªæœ‰åœ¨éœ€è¦ä¸åŒå­¦ä¹ ç‡æ—¶æ‰éœ€è¦æ‰‹åŠ¨åˆ†å±‚

4. **ä¼˜åŒ–å™¨çš„ä½œç”¨**ï¼š
   - ä¸ºæ¯ä¸ªå‚æ•°ç»´æŠ¤ä¼˜åŒ–çŠ¶æ€ï¼ˆåŠ¨é‡ã€å­¦ä¹ ç‡ç­‰ï¼‰
   - åœ¨ `optimizer.step()` æ—¶æ›´æ–°å‚æ•°

---

### å®Œæ•´è®­ç»ƒå¾ªç¯ç¤ºä¾‹

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“ä¾‹å­è¿½è¸ª**ä¸€æ¬¡å®Œæ•´çš„è¿­ä»£**ï¼š

```python
# ========== åˆå§‹åŒ– ==========
# å‡è®¾è¯æ±‡è¡¨å¤§å° 1000ï¼ŒåµŒå…¥ç»´åº¦ 32
model = Word2Vec(1000, 32)
optimizer = optim.AdamW(model.parameters(), lr=0.01)

# ========== ä¸€æ¬¡è¿­ä»£ ==========
# æ ·æœ¬ï¼š(center_word=5 "cat", context_word=10 "dog")

# æ­¥éª¤1ï¼šå‡†å¤‡æ•°æ®
center_tensor = torch.tensor([5], dtype=torch.long)
context_tensor = torch.tensor([10], dtype=torch.long)
neg_tensor = torch.tensor([[20, 30, 40, 50, 60]], dtype=torch.long)

# æ­¥éª¤2ï¼šæ¢¯åº¦æ¸…é›¶
optimizer.zero_grad()
# center_embed.weight.grad = zeros(1000, 32)
# outside_embed.weight.grad = zeros(1000, 32)

# æ­¥éª¤3ï¼šå‰å‘ä¼ æ’­
v_c = model.center_embed(center_tensor)      # å½¢çŠ¶ï¼š(1, 32)
u_o = model.outside_embed(context_tensor)    # å½¢çŠ¶ï¼š(1, 32)
u_neg = model.outside_embed(neg_tensor)      # å½¢çŠ¶ï¼š(1, 5, 32)

positive_scores = torch.sum(u_o * v_c, dim=1)  # [2.5]
negative_scores = torch.bmm(v_c.unsqueeze(1), u_neg.transpose(1, 2)).squeeze(1)  # [[1.0, 0.5, -0.5, -1.0, -2.0]]

# æ­¥éª¤4ï¼šè®¡ç®—æŸå¤±
loss = loss_fn(positive_scores, negative_scores)  # 3.279

# æ­¥éª¤5ï¼šåå‘ä¼ æ’­
loss.backward()
# PyTorch è‡ªåŠ¨è®¡ç®—ï¼š
# - center_embed.weight.grad[5] = [0.05, -0.02, ..., -0.03]
# - outside_embed.weight.grad[10] = [-0.03, 0.01, ..., 0.01]
# - outside_embed.weight.grad[20] = [0.02, -0.01, ..., -0.02]
# - outside_embed.weight.grad[30] = [0.01, 0.02, ..., 0.01]
# - ... (å…¶ä»–è´Ÿæ ·æœ¬)

# æ­¥éª¤6ï¼šæ›´æ–°å‚æ•°
optimizer.step()
# AdamW ç®—æ³•æ›´æ–°ï¼š
# - center_embed.weight.data[5] -= 0.01 * grad[5]
# - outside_embed.weight.data[10] -= 0.01 * grad[10]
# - outside_embed.weight.data[20] -= 0.01 * grad[20]
# - ... (å…¶ä»–è¢«ä½¿ç”¨çš„è¡Œ)

# ========== ä¸‹ä¸€æ¬¡è¿­ä»£ ==========
# å¤„ç†ä¸‹ä¸€ä¸ªæ ·æœ¬ï¼Œé‡å¤ä¸Šè¿°è¿‡ç¨‹
# æ³¨æ„ï¼šæ¯æ¬¡è¿­ä»£åªæ›´æ–°è¢«ä½¿ç”¨çš„å‚æ•°è¡Œ
```

---

### è®­ç»ƒè¿‡ç¨‹çš„å®è§‚ç†è§£

**æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„ç›®æ ‡**ï¼š

1. **æœ€å¤§åŒ–æ­£æ ·æœ¬ç›¸ä¼¼åº¦**ï¼š
   - è®©å…±ç°çš„è¯ï¼ˆå¦‚ "cat" å’Œ "dog"ï¼‰çš„å‘é‡ç›¸ä¼¼
   - é€šè¿‡æ›´æ–° `center_embed[5]` å’Œ `outside_embed[10]` å®ç°

2. **æœ€å°åŒ–è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦**ï¼š
   - è®©ä¸å…±ç°çš„è¯ï¼ˆå¦‚ "cat" å’Œéšæœºè¯ï¼‰çš„å‘é‡ä¸ç›¸ä¼¼
   - é€šè¿‡æ›´æ–° `center_embed[5]` å’Œ `outside_embed[20, 30, ...]` å®ç°

3. **ä¸¤ä¸ª Embedding å±‚ååŒ**ï¼š
   - `center_embed` å­¦ä¹ "ä½œä¸ºä¸­å¿ƒè¯"çš„è¡¨ç¤º
   - `outside_embed` å­¦ä¹ "ä½œä¸ºä¸Šä¸‹æ–‡è¯"çš„è¡¨ç¤º
   - ä¸¤è€…é€šè¿‡æŸå¤±å‡½æ•°å…±åŒä¼˜åŒ–

**è®­ç»ƒå®Œæˆåçš„ç»“æœ**ï¼š
- è¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¿‘
- ä¾‹å¦‚ï¼š"cat" å’Œ "dog" çš„å‘é‡ç›¸ä¼¼åº¦é«˜
- "cat" å’Œ "airplane" çš„å‘é‡ç›¸ä¼¼åº¦ä½

---

### å…³é”®è¦ç‚¹æ€»ç»“

1. **`optimizer.zero_grad()`**ï¼š
   - æ¸…é›¶æ¢¯åº¦ï¼Œé˜²æ­¢æ¢¯åº¦ç´¯ç§¯
   - å¿…é¡»åœ¨æ¯æ¬¡è¿­ä»£å‰è°ƒç”¨

2. **`model(...)`**ï¼š
   - å‰å‘ä¼ æ’­ï¼Œä¸¤ä¸ª Embedding å±‚ååŒå·¥ä½œ
   - `center_embed` ç”¨äºä¸­å¿ƒè¯ï¼Œ`outside_embed` ç”¨äºä¸Šä¸‹æ–‡è¯å’Œè´Ÿæ ·æœ¬

3. **`loss.backward()`**ï¼š
   - è‡ªåŠ¨æ±‚å¯¼ï¼Œè®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦
   - ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼Œæ²¿ç€è®¡ç®—å›¾åå‘ä¼ æ’­
   - æ¢¯åº¦å­˜å‚¨åœ¨ `param.grad` ä¸­

4. **`optimizer.step()`**ï¼š
   - æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°
   - AdamW ç®—æ³•è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
   - åªæ›´æ–°è¢«ä½¿ç”¨çš„å‚æ•°è¡Œ

5. **ä¸¤ä¸ª Embedding å±‚çš„ååŒ**ï¼š
   - å…±äº«è¯æ±‡è¡¨ï¼Œä½†å­¦ä¹ ä¸åŒè§’åº¦
   - é€šè¿‡æŸå¤±å‡½æ•°å…±åŒä¼˜åŒ–
   - æœ€ç»ˆé€šå¸¸ä½¿ç”¨ `center_embed` ä½œä¸ºè¯å‘é‡

---

## äº”ã€æ•´ä½“æµç¨‹æ€»ç»“

### Word2Vec Skip-Gram æ¨¡å‹çš„å®Œæ•´æµç¨‹ï¼š

1. **æ•°æ®å‡†å¤‡**ï¼š
   - æ–‡æœ¬ â†’ åˆ†è¯ â†’ è¯æ±‡è¡¨ â†’ è¯ç´¢å¼•æ˜ å°„
   - åˆ›å»º (ä¸­å¿ƒè¯, ä¸Šä¸‹æ–‡è¯) å¯¹

2. **æ¨¡å‹åˆå§‹åŒ–**ï¼š
   - åˆ›å»ºä¸¤ä¸ª Embedding çŸ©é˜µï¼ˆä¸­å¿ƒè¯å’Œä¸Šä¸‹æ–‡è¯ï¼‰
   - éšæœºåˆå§‹åŒ–å‚æ•°

3. **è®­ç»ƒå¾ªç¯**ï¼ˆå¯¹æ¯ä¸ªæ ·æœ¬å¯¹ï¼‰ï¼š
   ```
   a. è¾“å…¥ï¼šä¸­å¿ƒè¯ç´¢å¼•ã€ä¸Šä¸‹æ–‡è¯ç´¢å¼•ã€è´Ÿæ ·æœ¬ç´¢å¼•
   b. Embedding æŸ¥æ‰¾ï¼š
      - ä¸­å¿ƒè¯ç´¢å¼• â†’ ä¸­å¿ƒè¯å‘é‡ v_c (32ç»´)
      - ä¸Šä¸‹æ–‡è¯ç´¢å¼• â†’ ä¸Šä¸‹æ–‡è¯å‘é‡ u_o (32ç»´)
      - è´Ÿæ ·æœ¬ç´¢å¼• â†’ è´Ÿæ ·æœ¬å‘é‡ u_neg (5Ã—32ç»´)
   
   c. è®¡ç®—åˆ†æ•°ï¼š
      - æ­£æ ·æœ¬åˆ†æ•° = v_c Â· u_o (ç‚¹ç§¯)
      - è´Ÿæ ·æœ¬åˆ†æ•° = v_c Â· u_neg (æ‰¹é‡ç‚¹ç§¯)
   
   d. è®¡ç®—æŸå¤±ï¼š
      - æ­£æ ·æœ¬æŸå¤± = -log(sigmoid(æ­£æ ·æœ¬åˆ†æ•°))
      - è´Ÿæ ·æœ¬æŸå¤± = -sum(log(sigmoid(-è´Ÿæ ·æœ¬åˆ†æ•°)))
      - æ€»æŸå¤± = mean(æ­£æ ·æœ¬æŸå¤± + è´Ÿæ ·æœ¬æŸå¤±)
   
   e. åå‘ä¼ æ’­ï¼š
      - è®¡ç®—æ¢¯åº¦
      - æ›´æ–° Embedding çŸ©é˜µå‚æ•°
   ```

4. **è®­ç»ƒç›®æ ‡**ï¼š
   - è®©å…±ç°çš„è¯å‘é‡ç›¸ä¼¼ï¼ˆç‚¹ç§¯å¤§ï¼‰
   - è®©ä¸å…±ç°çš„è¯å‘é‡ä¸ç›¸ä¼¼ï¼ˆç‚¹ç§¯å°ï¼‰
   - æœ€ç»ˆï¼šè¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¿‘

### Embedding å±‚çš„ä½œç”¨æ€»ç»“ï¼š

**è¾“å…¥**ï¼šè¯çš„ç´¢å¼•ï¼ˆæ•´æ•°ï¼Œå¦‚ 5ï¼‰
**è¾“å‡º**ï¼šè¯çš„å‘é‡è¡¨ç¤ºï¼ˆæµ®ç‚¹æ•°æ•°ç»„ï¼Œå¦‚ 32 ç»´å‘é‡ï¼‰

**æœ¬è´¨**ï¼šä¸€ä¸ªå¯å­¦ä¹ çš„æŸ¥æ‰¾è¡¨
- è®­ç»ƒå‰ï¼šéšæœºåˆå§‹åŒ–
- è®­ç»ƒåï¼šæ¯ä¸ªè¯éƒ½æœ‰ä¸€ä¸ªæœ‰æ„ä¹‰çš„å‘é‡è¡¨ç¤º
- ç›¸ä¼¼è¯çš„å‘é‡åœ¨ç©ºé—´ä¸­è·ç¦»è¿‘
- å¯ä»¥ç”¨å‘é‡è¿ç®—ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰è¡¡é‡è¯ä¹‰ç›¸ä¼¼åº¦

---

## å…­ã€å…³é”®æ¦‚å¿µå›¾è§£

### Embedding å±‚å·¥ä½œåŸç†ï¼š
```
è¯æ±‡è¡¨ï¼š['cat', 'dog', 'car', 'house', ...]
ç´¢å¼•ï¼š   [0,    1,     2,     3,      ...]

Embedding çŸ©é˜µ (vocab_size Ã— embedding_dim):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 0: [0.1, 0.2, ..., 0.3] â”‚ â† 'cat' çš„å‘é‡
â”‚ 1: [0.4, 0.1, ..., 0.2] â”‚ â† 'dog' çš„å‘é‡
â”‚ 2: [0.5, 0.3, ..., 0.1] â”‚ â† 'car' çš„å‘é‡
â”‚ 3: [0.2, 0.4, ..., 0.5] â”‚ â† 'house' çš„å‘é‡
â”‚ ...                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥ç´¢å¼• [1] â†’ è¾“å‡ºå‘é‡ [0.4, 0.1, ..., 0.2] (dog çš„å‘é‡)
```

### å‰å‘ä¼ æ’­æµç¨‹ï¼š
```
è¾“å…¥ï¼š
  center_words = [5]
  context_words = [10]
  negative_samples = [[20, 30, 40, 50, 60]]

æ­¥éª¤1ï¼šEmbedding æŸ¥æ‰¾
  v_c = center_embed[5]   â†’ (1, 32)
  u_o = outside_embed[10] â†’ (1, 32)
  u_neg = outside_embed[[20,30,40,50,60]] â†’ (1, 5, 32)

æ­¥éª¤2ï¼šè®¡ç®—ç›¸ä¼¼åº¦
  positive_scores = v_c Â· u_o = [2.5]
  negative_scores = v_c Â· u_neg = [[1.0, 0.5, -0.5, -1.0, -2.0]]

æ­¥éª¤3ï¼šè®¡ç®—æŸå¤±
  loss = -log(sigmoid(2.5)) - sum(log(sigmoid(-[1.0, 0.5, ...])))
       = 0.08 + 2.3 = 2.38

æ­¥éª¤4ï¼šåå‘ä¼ æ’­æ›´æ–°å‚æ•°
  è°ƒæ•´ Embedding çŸ©é˜µï¼Œè®©æŸå¤±å‡å°
```

---

## ä¸ƒã€å¸¸è§é—®é¢˜è§£ç­”

### Q1: ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ª Embedding å±‚ï¼Ÿ
**A**: 
- ä¸¤ä¸ªçŸ©é˜µå¯ä»¥å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºè§’åº¦
- è®­ç»ƒæ›´ç¨³å®šï¼Œæ”¶æ•›æ›´å¿«
- æœ€ç»ˆé€šå¸¸åªä½¿ç”¨ `center_embed` ä½œä¸ºè¯å‘é‡

### Q2: Embedding å±‚çš„å‚æ•°æœ‰å¤šå°‘ï¼Ÿ
**A**: 
- `center_embed`: `vocab_size Ã— embedding_dim`
- `outside_embed`: `vocab_size Ã— embedding_dim`
- æ€»å‚æ•°ï¼š`2 Ã— vocab_size Ã— embedding_dim`
- ä¾‹å¦‚ï¼š1000 è¯ Ã— 32 ç»´ Ã— 2 = 64,000 ä¸ªå‚æ•°

### Q3: è´Ÿé‡‡æ ·ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ
**A**: 
- å¦‚æœä¸ä½¿ç”¨è´Ÿé‡‡æ ·ï¼Œéœ€è¦è®¡ç®—æ‰€æœ‰è¯çš„åˆ†æ•°ï¼ˆè®¡ç®—é‡å¤§ï¼‰
- è´Ÿé‡‡æ ·åªè®¡ç®— k ä¸ªéšæœºè¯çš„åˆ†æ•°ï¼Œå¤§å¤§å‡å°‘è®¡ç®—é‡
- åŒæ—¶è®©æ¨¡å‹å­¦ä¼šåŒºåˆ†"æ˜¯ä¸Šä¸‹æ–‡"å’Œ"ä¸æ˜¯ä¸Šä¸‹æ–‡"

### Q4: è®­ç»ƒåå¦‚ä½•ä½¿ç”¨è¯å‘é‡ï¼Ÿ
**A**: 
```python
# è·å–æŸä¸ªè¯çš„å‘é‡
word_idx = word_to_ix['cat']
word_vector = model.center_embed.weight.data[word_idx]  # (32,)

# è®¡ç®—ä¸¤ä¸ªè¯çš„ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
vec1 = model.center_embed.weight.data[word_to_ix['cat']]
vec2 = model.center_embed.weight.data[word_to_ix['dog']]
similarity = torch.cosine_similarity(vec1, vec2, dim=0)
```

---

å¸Œæœ›è¿™ä»½è¯¦ç»†è§£è¯»èƒ½å¸®åŠ©ä½ ç†è§£ Word2Vec å’Œ PyTorch çš„æ¯ä¸€è¡Œä»£ç ï¼

