# Word2Vec Skip-Gram æ¨¡å‹å®ç° - å­¦ä¹ ä¸å®ç°æŒ‡å—

## ğŸ“š ä¸€ã€éœ€è¦å­¦ä¹ çš„å†…å®¹

### 1.1 Word2Vec ç†è®ºåŸºç¡€

#### 1.1.1 Skip-Gram æ¨¡å‹åŸç†
- **æ ¸å¿ƒæ€æƒ³**: ç»™å®šä¸­å¿ƒè¯ï¼Œé¢„æµ‹å…¶ä¸Šä¸‹æ–‡è¯ï¼ˆä¸CBOWç›¸åï¼ŒCBOWæ˜¯ç»™å®šä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯ï¼‰
- **è®­ç»ƒç›®æ ‡**: å­¦ä¹ è¯å‘é‡ï¼Œä½¿å¾—è¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»è¾ƒè¿‘
- **ä¸ºä»€ä¹ˆéœ€è¦**: å°†ç¦»æ•£çš„è¯è½¬æ¢ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤ºï¼Œä¾¿äºè¿›è¡Œæ•°å­¦è¿ç®—å’Œæœºå™¨å­¦ä¹ 

#### 1.1.2 è´Ÿé‡‡æ ·ï¼ˆNegative Samplingï¼‰æœºåˆ¶
- **é—®é¢˜**: åŸå§‹Skip-Graméœ€è¦å¯¹æ•´ä¸ªè¯æ±‡è¡¨è¿›è¡Œsoftmaxï¼Œè®¡ç®—é‡å¤§
- **è§£å†³æ–¹æ¡ˆ**: åªå¯¹å°‘æ•°è´Ÿæ ·æœ¬ï¼ˆä¸æ˜¯ä¸Šä¸‹æ–‡è¯çš„è¯ï¼‰è¿›è¡Œé‡‡æ ·æ›´æ–°
- **ä¼˜åŠ¿**: 
  - å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦ï¼ˆä»O(V)é™åˆ°O(K+1)ï¼ŒVæ˜¯è¯æ±‡è¡¨å¤§å°ï¼ŒKæ˜¯è´Ÿæ ·æœ¬æ•°ï¼‰
  - åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹
  - æ•ˆæœä¸åŸå§‹æ–¹æ³•ç›¸å½“

#### 1.1.3 æŸå¤±å‡½æ•°ç†è§£ï¼ˆé‡ç‚¹ï¼ï¼‰

**å…¬å¼ (Equation 8):**
```
L = -log(Ïƒ(u_o^T v_c)) - Î£_{i=1}^{K} log(Ïƒ(-u_{w_i}^T v_c))
```

**å…¬å¼è§£è¯»:**
- `v_c`: ä¸­å¿ƒè¯cçš„ä¸­å¿ƒå‘é‡ï¼ˆcenter embeddingï¼‰
- `u_o`: æ­£ä¸Šä¸‹æ–‡è¯oçš„å¤–éƒ¨å‘é‡ï¼ˆoutside embeddingï¼‰
- `u_{w_i}`: ç¬¬iä¸ªè´Ÿæ ·æœ¬çš„å¤–éƒ¨å‘é‡
- `Ïƒ(x)`: sigmoidå‡½æ•°ï¼ŒÏƒ(x) = 1/(1+e^(-x))
- `u_o^T v_c`: å‘é‡ç‚¹ç§¯ï¼Œè¡¨ç¤ºä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦

**æŸå¤±å‡½æ•°å«ä¹‰:**
- ç¬¬ä¸€éƒ¨åˆ† `-log(Ïƒ(u_o^T v_c))`: æœ€å¤§åŒ–æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦
- ç¬¬äºŒéƒ¨åˆ† `-Î£ log(Ïƒ(-u_{w_i}^T v_c))`: æœ€å°åŒ–è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼åº¦
- æ€»ä½“ç›®æ ‡: è®©ä¸­å¿ƒè¯ä¸æ­£ä¸Šä¸‹æ–‡è¯ç›¸ä¼¼ï¼Œä¸è´Ÿæ ·æœ¬ä¸ç›¸ä¼¼

### 1.2 PyTorch åŸºç¡€çŸ¥è¯†

#### 1.2.1 æ ¸å¿ƒç»„ä»¶

**`torch.nn.Module`**
- æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å—çš„åŸºç±»
- å¿…é¡»å®ç° `__init__()` å’Œ `forward()` æ–¹æ³•
- ä¼šè‡ªåŠ¨è·Ÿè¸ªæ‰€æœ‰å‚æ•°ï¼ˆé€šè¿‡ `nn.Parameter` æˆ– `nn.Module` çš„å­æ¨¡å—ï¼‰

**`torch.nn.Embedding`**
```python
# åˆ›å»ºåµŒå…¥å±‚
embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=dim)
# ä½¿ç”¨ï¼šç»™å®šè¯çš„ç´¢å¼•ï¼Œè¿”å›å¯¹åº”çš„å‘é‡
word_vector = embedding(torch.tensor([word_idx]))
```

**å¼ é‡æ“ä½œ**
- çŸ©é˜µä¹˜æ³•: `torch.matmul(a, b)` æˆ– `a @ b`
- ç‚¹ç§¯: `torch.sum(a * b)` æˆ– `(a * b).sum()`
- Sigmoid: `torch.sigmoid(x)` æˆ– `torch.nn.functional.sigmoid(x)`
- Log: `torch.log(x)`
- æ±‚å’Œ: `torch.sum(x)` æˆ– `x.sum()`

**è®­ç»ƒæµç¨‹**
```python
# 1. å‰å‘ä¼ æ’­
output = model(input)
loss = loss_fn(output, target)

# 2. åå‘ä¼ æ’­
loss.backward()

# 3. å‚æ•°æ›´æ–°
optimizer.step()

# 4. æ¢¯åº¦æ¸…é›¶ï¼ˆé‡è¦ï¼ï¼‰
optimizer.zero_grad()
```

### 1.3 è´Ÿé‡‡æ ·å®ç°

**é‡‡æ ·åˆ†å¸ƒæ„å»º**
- é€šå¸¸ä½¿ç”¨è¯é¢‘çš„ 3/4 æ¬¡æ–¹: `prob = count^(3/4) / sum(count^(3/4))`
- ä½¿ç”¨ `torch.multinomial()` è¿›è¡Œé‡‡æ ·
- ç¡®ä¿è´Ÿæ ·æœ¬ä¸æ˜¯æ­£æ ·æœ¬æœ¬èº«

## ğŸ“ äºŒã€å¦‚ä½•å­¦ä¹ 

### 2.1 ç†è®ºå­¦ä¹ èµ„æº

**åœ¨çº¿è¯¾ç¨‹**
- **CS224N (Stanford)**: è¯¾ç¨‹ç½‘ç«™æœ‰è¯¦ç»†çš„Word2Vecè®²ä¹‰å’Œè§†é¢‘
- **Deep Learning Specialization (Coursera)**: Andrew Ngçš„è¯¾ç¨‹åŒ…å«è¯åµŒå…¥ç« èŠ‚

**åšå®¢å’Œæ•™ç¨‹**
- æœç´¢å…³é”®è¯: "Word2Vec Skip-Gram Negative Sampling explained"
- æ¨è: "The Illustrated Word2Vec" (Jay Alammarçš„åšå®¢)
- PyTorchå®˜æ–¹æ–‡æ¡£: `torch.nn.Embedding` ç¤ºä¾‹

**è®ºæ–‡**
- Mikolov et al. (2013): "Distributed Representations of Words and Phrases"
- è¿™æ˜¯Word2Vecçš„åŸå§‹è®ºæ–‡ï¼Œæœ‰è¯¦ç»†çš„æ•°å­¦æ¨å¯¼

### 2.2 å®è·µå­¦ä¹ è·¯å¾„

**ç¬¬ä¸€æ­¥: ç†è§£PyTorchåŸºç¡€**
```python
# ç»ƒä¹ 1: åˆ›å»ºç®€å•çš„åµŒå…¥å±‚
import torch
import torch.nn as nn

vocab_size = 1000
embed_dim = 50
embedding = nn.Embedding(vocab_size, embed_dim)

# è¾“å…¥è¯çš„ç´¢å¼•
word_idx = torch.tensor([0, 1, 2])
word_vectors = embedding(word_idx)  # shape: (3, 50)
```

**ç¬¬äºŒæ­¥: ç†è§£è´Ÿé‡‡æ ·**
```python
# ç»ƒä¹ 2: ç†è§£é‡‡æ ·è¿‡ç¨‹
import torch

# å‡è®¾æœ‰è¯é¢‘åˆ†å¸ƒ
word_counts = torch.tensor([100, 50, 30, 20, 10])
# è®¡ç®—é‡‡æ ·æ¦‚ç‡ï¼ˆ3/4æ¬¡æ–¹ï¼‰
probs = word_counts ** 0.75
probs = probs / probs.sum()

# é‡‡æ ·5ä¸ªè´Ÿæ ·æœ¬
negative_samples = torch.multinomial(probs, 5, replacement=True)
```

**ç¬¬ä¸‰æ­¥: ç†è§£æŸå¤±å‡½æ•°**
```python
# ç»ƒä¹ 3: æ‰‹åŠ¨è®¡ç®—æŸå¤±å‡½æ•°çš„ä¸€ä¸ªä¾‹å­
import torch
import torch.nn.functional as F

# å‡è®¾çš„å‘é‡
v_c = torch.tensor([1.0, 2.0])  # ä¸­å¿ƒè¯å‘é‡
u_o = torch.tensor([1.5, 1.0])  # æ­£æ ·æœ¬å‘é‡
u_neg1 = torch.tensor([-1.0, -1.0])  # è´Ÿæ ·æœ¬1
u_neg2 = torch.tensor([-0.5, -2.0])  # è´Ÿæ ·æœ¬2

# è®¡ç®—æ­£æ ·æœ¬çš„logæ¦‚ç‡
pos_score = torch.dot(u_o, v_c)  # ç‚¹ç§¯
pos_loss = -torch.log(torch.sigmoid(pos_score))

# è®¡ç®—è´Ÿæ ·æœ¬çš„logæ¦‚ç‡
neg_score1 = torch.dot(u_neg1, v_c)
neg_score2 = torch.dot(u_neg2, v_c)
neg_loss = -torch.log(torch.sigmoid(-neg_score1)) - torch.log(torch.sigmoid(-neg_score2))

# æ€»æŸå¤±
total_loss = pos_loss + neg_loss
```

## ğŸ’» ä¸‰ã€å…·ä½“ä»£ç å®ç°æ­¥éª¤

### 3.1 å®ç°é¡ºåºå»ºè®®

æŒ‰ç…§ä»¥ä¸‹é¡ºåºå®ç°ï¼Œæ¯ä¸€æ­¥éƒ½å»ºç«‹åœ¨å‰ä¸€æ­¥çš„åŸºç¡€ä¸Šï¼š

1. **TODO 1**: å®ç° `Word2Vec` æ¨¡å‹ç±»
2. **TODO 3**: å®ç° `NegativeSampler` ç±»
3. **TODO 2**: å®ç°æŸå¤±å‡½æ•°
4. **TODO 4**: å®Œæˆè®­ç»ƒå¾ªç¯

### 3.2 TODO 1: å®ç° Word2Vec æ¨¡å‹ç±»

**ä½ç½®**: `class Word2Vec(nn.Module):` (ç¬¬53-55è¡Œ)

**éœ€è¦å®ç°çš„å†…å®¹:**

```python
class Word2Vec(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(Word2Vec, self).__init__()
        # ä¸­å¿ƒè¯åµŒå…¥å±‚ï¼ˆcenter embeddingsï¼‰
        self.in_embed = nn.Embedding(vocab_size, embedding_dim)
        # å¤–éƒ¨è¯åµŒå…¥å±‚ï¼ˆoutside embeddingsï¼‰
        self.out_embed = nn.Embedding(vocab_size, embedding_dim)
        
        # åˆå§‹åŒ–æƒé‡ï¼ˆå¯é€‰ï¼Œä½†æ¨èï¼‰
        # ä½¿ç”¨è¾ƒå°çš„éšæœºå€¼åˆå§‹åŒ–
        self.in_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)
        self.out_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)
    
    def forward(self, center_idx, context_idx, neg_samples):
        """
        å‰å‘ä¼ æ’­
        
        å‚æ•°:
            center_idx: ä¸­å¿ƒè¯ç´¢å¼• (æ ‡é‡æˆ–1Då¼ é‡)
            context_idx: æ­£ä¸Šä¸‹æ–‡è¯ç´¢å¼• (æ ‡é‡æˆ–1Då¼ é‡)
            neg_samples: è´Ÿæ ·æœ¬ç´¢å¼• (1Då¼ é‡ï¼Œé•¿åº¦ä¸ºK)
        
        è¿”å›:
            center_emb: ä¸­å¿ƒè¯å‘é‡
            context_emb: æ­£ä¸Šä¸‹æ–‡è¯å‘é‡
            neg_embs: è´Ÿæ ·æœ¬å‘é‡
        """
        # è·å–ä¸­å¿ƒè¯å‘é‡ v_c
        center_emb = self.in_embed(center_idx)  # shape: (1, embedding_dim)
        
        # è·å–æ­£ä¸Šä¸‹æ–‡è¯å‘é‡ u_o
        context_emb = self.out_embed(context_idx)  # shape: (1, embedding_dim)
        
        # è·å–è´Ÿæ ·æœ¬å‘é‡ u_{w_i}
        neg_embs = self.out_embed(neg_samples)  # shape: (K, embedding_dim)
        
        return center_emb, context_emb, neg_embs
```

**å…³é”®ç‚¹:**
- ä½¿ç”¨ `nn.Embedding` åˆ›å»ºä¸¤ä¸ªåµŒå…¥å±‚
- `in_embed` ç”¨äºä¸­å¿ƒè¯ï¼ˆcenter wordï¼‰
- `out_embed` ç”¨äºä¸Šä¸‹æ–‡è¯å’Œè´Ÿæ ·æœ¬ï¼ˆoutside wordsï¼‰
- `forward` æ–¹æ³•è¿”å›æ‰€æœ‰éœ€è¦çš„å‘é‡

### 3.3 TODO 3: å®ç° NegativeSampler ç±»

**ä½ç½®**: `class NegativeSampler:` (ç¬¬57-59è¡Œ)

**éœ€è¦å®ç°çš„å†…å®¹:**

```python
class NegativeSampler:
    def __init__(self, word_counts):
        """
        åˆå§‹åŒ–è´Ÿé‡‡æ ·å™¨
        
        å‚æ•°:
            word_counts: Counterå¯¹è±¡ï¼ŒåŒ…å«æ¯ä¸ªè¯çš„é¢‘ç‡
        """
        # å°†Counterè½¬æ¢ä¸ºåˆ—è¡¨ï¼Œä¿æŒç´¢å¼•é¡ºåº
        # æ³¨æ„ï¼šword_countsçš„é”®æ˜¯è¯ï¼Œå€¼æ˜¯é¢‘ç‡
        self.word_counts = word_counts
        
        # æ„å»ºé‡‡æ ·åˆ†å¸ƒ
        # ä½¿ç”¨è¯é¢‘çš„3/4æ¬¡æ–¹ï¼ˆè¿™æ˜¯Word2Vecè®ºæ–‡ä¸­çš„æ ‡å‡†åšæ³•ï¼‰
        counts_list = []
        for word, count in sorted(word_counts.items()):
            counts_list.append(count)
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶è®¡ç®—é‡‡æ ·æ¦‚ç‡
        counts_tensor = torch.tensor(counts_list, dtype=torch.float)
        # è®¡ç®—è¯é¢‘çš„3/4æ¬¡æ–¹
        probs = counts_tensor ** 0.75
        # å½’ä¸€åŒ–å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
        self.sampling_probs = probs / probs.sum()
    
    def get_negative_samples(self, context_word_idx, k):
        """
        è·å–è´Ÿæ ·æœ¬
        
        å‚æ•°:
            context_word_idx: æ­£ä¸Šä¸‹æ–‡è¯çš„ç´¢å¼•ï¼ˆè¦æ’é™¤ï¼‰
            k: éœ€è¦çš„è´Ÿæ ·æœ¬æ•°é‡
        
        è¿”å›:
            neg_samples: åŒ…å«kä¸ªè´Ÿæ ·æœ¬ç´¢å¼•çš„1Då¼ é‡
        """
        # ä½¿ç”¨multinomialè¿›è¡Œé‡‡æ ·
        # replacement=True å…è®¸é‡å¤é‡‡æ ·
        neg_samples = torch.multinomial(
            self.sampling_probs, 
            k, 
            replacement=True
        )
        
        # ç¡®ä¿è´Ÿæ ·æœ¬ä¸åŒ…å«æ­£æ ·æœ¬æœ¬èº«ï¼ˆè™½ç„¶æ¦‚ç‡å¾ˆå°ï¼Œä½†æœ€å¥½æ£€æŸ¥ï¼‰
        # å¦‚æœé‡‡æ ·åˆ°äº†æ­£æ ·æœ¬ï¼Œé‡æ–°é‡‡æ ·
        while torch.any(neg_samples == context_word_idx):
            mask = neg_samples == context_word_idx
            # åªé‡æ–°é‡‡æ ·é‚£äº›ç­‰äºæ­£æ ·æœ¬çš„ä½ç½®
            new_samples = torch.multinomial(
                self.sampling_probs,
                mask.sum().item(),
                replacement=True
            )
            neg_samples[mask] = new_samples
        
        return neg_samples
```

**å…³é”®ç‚¹:**
- ä½¿ç”¨è¯é¢‘çš„3/4æ¬¡æ–¹æ„å»ºé‡‡æ ·åˆ†å¸ƒï¼ˆé¿å…è¿‡åº¦é‡‡æ ·é«˜é¢‘è¯ï¼‰
- ä½¿ç”¨ `torch.multinomial()` è¿›è¡Œé‡‡æ ·
- ç¡®ä¿è´Ÿæ ·æœ¬ä¸åŒ…å«æ­£æ ·æœ¬æœ¬èº«

### 3.4 TODO 2: å®ç°æŸå¤±å‡½æ•°

**ä½ç½®**: ä¸»ç¨‹åºä¸­ `loss_fn = None` (ç¬¬113è¡Œ)

**å®ç°æ–¹å¼1: ä½œä¸ºç‹¬ç«‹å‡½æ•°**

```python
def negative_sampling_loss(center_emb, context_emb, neg_embs):
    """
    è®¡ç®—è´Ÿé‡‡æ ·æŸå¤±
    
    å‚æ•°:
        center_emb: ä¸­å¿ƒè¯å‘é‡ (1, embedding_dim)
        context_emb: æ­£ä¸Šä¸‹æ–‡è¯å‘é‡ (1, embedding_dim)
        neg_embs: è´Ÿæ ·æœ¬å‘é‡ (K, embedding_dim)
    
    è¿”å›:
        loss: æ ‡é‡æŸå¤±å€¼
    """
    # è®¡ç®—æ­£æ ·æœ¬çš„åˆ†æ•°: u_o^T v_c
    pos_score = torch.sum(context_emb * center_emb, dim=1)  # (1,)
    # è®¡ç®—æ­£æ ·æœ¬çš„æŸå¤±: -log(Ïƒ(u_o^T v_c))
    pos_loss = -torch.log(torch.sigmoid(pos_score))
    
    # è®¡ç®—è´Ÿæ ·æœ¬çš„åˆ†æ•°: u_{w_i}^T v_c
    # center_emb: (1, embedding_dim), neg_embs: (K, embedding_dim)
    # ä½¿ç”¨çŸ©é˜µä¹˜æ³•: (K, embedding_dim) @ (embedding_dim, 1) = (K, 1)
    neg_scores = torch.matmul(neg_embs, center_emb.t())  # (K, 1)
    neg_scores = neg_scores.squeeze()  # (K,)
    
    # è®¡ç®—è´Ÿæ ·æœ¬çš„æŸå¤±: -Î£ log(Ïƒ(-u_{w_i}^T v_c))
    neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_scores)))
    
    # æ€»æŸå¤±
    total_loss = pos_loss + neg_loss
    
    return total_loss
```

**åœ¨ä¸»ç¨‹åºä¸­:**
```python
loss_fn = negative_sampling_loss
```

**å®ç°æ–¹å¼2: ä½œä¸ºæ¨¡å‹çš„æ–¹æ³•**

å¦‚æœä½ æƒ³è®©æŸå¤±å‡½æ•°æ›´ç´§å¯†åœ°ä¸æ¨¡å‹ç»“åˆï¼Œä¹Ÿå¯ä»¥å°†å…¶ä½œä¸º `Word2Vec` ç±»çš„æ–¹æ³•ï¼š

```python
class Word2Vec(nn.Module):
    # ... __init__ æ–¹æ³• ...
    
    def forward(self, center_idx, context_idx, neg_samples):
        # ... è·å–å‘é‡ ...
        return center_emb, context_emb, neg_embs
    
    def compute_loss(self, center_idx, context_idx, neg_samples):
        """è®¡ç®—æŸå¤±"""
        center_emb, context_emb, neg_embs = self.forward(
            center_idx, context_idx, neg_samples
        )
        
        # æ­£æ ·æœ¬æŸå¤±
        pos_score = torch.sum(context_emb * center_emb, dim=1)
        pos_loss = -torch.log(torch.sigmoid(pos_score))
        
        # è´Ÿæ ·æœ¬æŸå¤±
        neg_scores = torch.matmul(neg_embs, center_emb.t()).squeeze()
        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_scores)))
        
        return pos_loss + neg_loss
```

**å…³é”®ç‚¹:**
- æ­£æ ·æœ¬: `-log(Ïƒ(u_o^T v_c))`
- è´Ÿæ ·æœ¬: `-Î£ log(Ïƒ(-u_{w_i}^T v_c))`
- æ³¨æ„è´Ÿæ ·æœ¬ä¸­çš„è´Ÿå·ï¼ˆ`-u_{w_i}^T v_c`ï¼‰

### 3.5 TODO 4: å®Œæˆè®­ç»ƒå¾ªç¯

**ä½ç½®**: `train()` å‡½æ•°ä¸­ (ç¬¬61-76è¡Œ)

**éœ€è¦æ·»åŠ çš„ä»£ç :**

```python
def train(model, training_data, optimizer, loss_fn, epochs, negative_sampler, k_negative_samples=5):
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    
    for epoch in range(epochs):
        total_loss = 0
        random.shuffle(training_data)
        progress_bar = tqdm(training_data, desc=f"Epoch {epoch + 1}/{epochs}")
        
        for i, (center_word_idx, context_word_idx) in enumerate(progress_bar):
            # 1. æ¢¯åº¦æ¸…é›¶
            optimizer.zero_grad()
            
            # 2. è·å–è´Ÿæ ·æœ¬
            neg_sample_indices = negative_sampler.get_negative_samples(
                context_word_idx, 
                k_negative_samples
            )
            
            # 3. è½¬æ¢ä¸ºå¼ é‡
            center_tensor = torch.tensor([center_word_idx], dtype=torch.long)
            context_tensor = torch.tensor([context_word_idx], dtype=torch.long)
            
            # 4. å‰å‘ä¼ æ’­ï¼šè·å–è¯å‘é‡
            center_emb, context_emb, neg_embs = model(
                center_tensor, 
                context_tensor, 
                neg_sample_indices
            )
            
            # 5. è®¡ç®—æŸå¤±
            loss = loss_fn(center_emb, context_emb, neg_embs)
            
            # 6. åå‘ä¼ æ’­
            loss.backward()
            
            # 7. å‚æ•°æ›´æ–°
            optimizer.step()
            
            # 8. ç´¯ç§¯æŸå¤±ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            total_loss += loss.item()
            
            # 9. æ›´æ–°è¿›åº¦æ¡
            if i % 1000 == 0:
                progress_bar.set_postfix({'loss': f'{total_loss / (i + 1):.4f}'})
        
        # 10. æ¯ä¸ªepochç»“æŸåæ‰“å°å¹³å‡æŸå¤±
        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(training_data):.4f}")
```

**å…³é”®ç‚¹:**
- æ¯ä¸ªbatchéƒ½è¦è°ƒç”¨ `optimizer.zero_grad()` æ¸…é›¶æ¢¯åº¦
- è°ƒç”¨ `model()` è¿›è¡Œå‰å‘ä¼ æ’­
- è°ƒç”¨ `loss.backward()` è¿›è¡Œåå‘ä¼ æ’­
- è°ƒç”¨ `optimizer.step()` æ›´æ–°å‚æ•°
- ä½¿ç”¨ `loss.item()` è·å–æ ‡é‡æŸå¤±å€¼

### 3.6 å…¶ä»–éœ€è¦ä¿®å¤çš„é—®é¢˜

**é—®é¢˜1: CUDAæ£€æŸ¥ä»£ç ï¼ˆç¬¬96è¡Œï¼‰**

å½“å‰ä»£ç ï¼š
```python
print("PyTorch version:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
```

ä¿®å¤ä¸ºï¼š
```python
print("PyTorch version:", torch.__version__)
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("CUDA device:", torch.cuda.get_device_name(0))
else:
    print("Using CPU")
```

**é—®é¢˜2: min_freqå‚æ•°ï¼ˆç¬¬99è¡Œï¼‰**

å½“å‰ä»£ç ä½¿ç”¨ `min_freq=1`ï¼Œè¿™ä¼šå¯¼è‡´è¯æ±‡è¡¨å¾ˆå¤§ã€‚å»ºè®®ï¼š
- å¦‚æœè¯­æ–™å¾ˆå°ï¼Œå¯ä»¥ä½¿ç”¨ `min_freq=1`
- å¦‚æœè¯­æ–™è¾ƒå¤§ï¼Œå»ºè®®ä½¿ç”¨ `min_freq=2` æˆ–æ›´é«˜

## ğŸ“ å››ã€å®Œæ•´å®ç°æ£€æŸ¥æ¸…å•

å®Œæˆå®ç°åï¼Œæ£€æŸ¥ä»¥ä¸‹é¡¹ç›®ï¼š

- [ ] `Word2Vec` ç±»æœ‰ä¸¤ä¸ª `nn.Embedding` å±‚
- [ ] `forward` æ–¹æ³•è¿”å›ä¸­å¿ƒè¯ã€ä¸Šä¸‹æ–‡è¯å’Œè´Ÿæ ·æœ¬çš„å‘é‡
- [ ] `NegativeSampler` ä½¿ç”¨è¯é¢‘çš„3/4æ¬¡æ–¹æ„å»ºé‡‡æ ·åˆ†å¸ƒ
- [ ] `get_negative_samples` è¿”å›æ­£ç¡®æ•°é‡çš„è´Ÿæ ·æœ¬
- [ ] æŸå¤±å‡½æ•°æ­£ç¡®å®ç°äº†å…¬å¼ (8)
- [ ] è®­ç»ƒå¾ªç¯åŒ…å«å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°
- [ ] æ¢¯åº¦æ¸…é›¶åœ¨æ¯ä¸ªbatchå¼€å§‹æ—¶æ‰§è¡Œ
- [ ] æŸå¤±å€¼æ­£ç¡®ç´¯ç§¯å’Œæ˜¾ç¤º

## ğŸš€ äº”ã€æµ‹è¯•å»ºè®®

1. **å°è§„æ¨¡æµ‹è¯•**: å…ˆç”¨å¾ˆå°çš„è¯æ±‡è¡¨ï¼ˆå¦‚10ä¸ªè¯ï¼‰æµ‹è¯•ä»£ç æ˜¯å¦èƒ½è¿è¡Œ
2. **æ£€æŸ¥æŸå¤±**: ç¡®ä¿æŸå¤±å€¼åœ¨åˆç†èŒƒå›´å†…ï¼ˆä¸åº”è¯¥ä¸ºNaNæˆ–Infï¼‰
3. **æ£€æŸ¥æ¢¯åº¦**: å¯ä»¥åœ¨è®­ç»ƒå‰æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨ï¼ˆ`model.in_embed.weight.grad`ï¼‰
4. **å¯è§†åŒ–**: è¿è¡Œ `visualize_embeddings` æŸ¥çœ‹è¯å‘é‡åˆ†å¸ƒ

## ğŸ“š å…­ã€å‚è€ƒèµ„æº

- **PyTorchå®˜æ–¹æ–‡æ¡£**: https://pytorch.org/docs/stable/index.html
- **Word2Vecè®ºæ–‡**: Mikolov et al. (2013)
- **CS224Nè¯¾ç¨‹**: https://web.stanford.edu/class/cs224n/
- **Jay Alammarçš„å¯è§†åŒ–åšå®¢**: http://jalammar.github.io/illustrated-word2vec/

---

**ç¥ä½ å®ç°é¡ºåˆ©ï¼** è®°ä½ï¼Œç†è§£åŸç†æ¯”ç›´æ¥å¤åˆ¶ä»£ç æ›´é‡è¦ã€‚å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ï¼š
1. æ‰“å°ä¸­é—´å˜é‡çš„å½¢çŠ¶å’Œå€¼æ¥è°ƒè¯•
2. æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨
3. éªŒè¯æŸå¤±å‡½æ•°è®¡ç®—æ˜¯å¦æ­£ç¡®

