# Word2Vec Skip-Gram 模型训练详细说明

## 概述

本代码实现了Word2Vec的Skip-Gram模型，使用负采样（Negative Sampling）进行训练。该实现严格按照题目要求，实现了以下四个核心组件：

1. **Word2Vec模型类**：包含两个嵌入层（center和outside向量）
2. **负采样损失函数**：实现Eq. (8)的损失函数
3. **负采样器**：从词汇表中采样K个负样本
4. **训练循环**：完整的前向传播、损失计算、反向传播流程

## 模型架构

### 双嵌入层设计

Word2Vec模型为每个词`w`学习两个向量：

- **`v_w`（center向量）**：当词`w`作为中心词时使用
- **`u_w`（outside向量）**：当词`w`作为上下文词时使用

```
模型结构：
center_embed: (vocab_size, embedding_dim) → v_w
outside_embed: (vocab_size, embedding_dim) → u_w
```

### 前向传播

对于给定的训练对`(center_word, context_word)`和K个负样本：

1. **获取嵌入向量**：
   - `v_c = center_embed[center_word]` - 中心词向量
   - `u_o = outside_embed[context_word]` - 正上下文词向量
   - `u_neg = outside_embed[negative_samples]` - 负样本向量

2. **计算得分**：
   - 正样本得分：`u_o^T v_c`（点积）
   - 负样本得分：`u_{w_i}^T v_c`（批量矩阵乘法）

## 损失函数（Eq. 8）

损失函数定义如下：

$$\mathcal{L} = - \log(\sigma(u_o^T v_c)) - \sum_{i=1}^{K} \log(\sigma(-u_{w_i}^T v_c))$$

### 损失函数组成

**第一项：`-log(σ(u_o^T v_c))`**
- 目标：最大化正样本对的相似度
- 当`u_o^T v_c`很大时，`σ(u_o^T v_c)`接近1，`-log(σ(...))`接近0
- 鼓励模型对正样本对赋予高概率

**第二项：`-∑log(σ(-u_{w_i}^T v_c))`**
- 目标：最小化负样本对的相似度
- 注意：`σ(-x) = 1 - σ(x)`，所以这等价于`-∑log(1 - σ(u_{w_i}^T v_c))`
- 当`u_{w_i}^T v_c`很小时，`σ(u_{w_i}^T v_c)`接近0，`σ(-u_{w_i}^T v_c)`接近1，损失小
- 鼓励模型对负样本对赋予低概率

### 数值稳定性

- 在计算`log(σ(...))`时添加小常数`1e-8`防止数值下溢
- 使用PyTorch的`sigmoid`函数确保数值稳定性

## 负采样策略

### 采样分布

使用一元语言模型分布，提升到0.75次幂：

$$P(w) = \frac{count(w)^{0.75}}{\sum_{w'} count(w')^{0.75}}$$

**为什么使用0.75次幂？**
- 经验值，来自Word2Vec原始论文
- 降低高频词的采样概率（如"the", "a"）
- 提高低频词的采样概率
- 平衡高频词和低频词的学习

### 实现细节

1. **构建分布**：
   - 获取所有词的频率
   - 计算`count^0.75`
   - 归一化得到采样概率

2. **采样过程**：
   - 使用`numpy.random.choice`进行加权随机采样
   - 允许重复采样（`replace=True`）
   - 返回K个词汇表索引

## 训练流程

### 数据准备阶段

1. **文本预处理**：
   - 转换为小写
   - 移除非字母字符
   - 规范化空白字符

2. **词汇表构建**：
   - 统计词频
   - 过滤低频词（`min_freq=1`，在本作业中保留所有词）
   - 创建`<unk>`token处理未知词
   - 构建词汇映射：`word_to_ix`和`ix_to_word`

3. **Skip-Gram数据生成**：
   - 对每个中心词，在窗口内收集上下文词
   - 生成`(center_word_idx, context_word_idx)`对
   - 跳过`<unk>`词

### 训练迭代

对于每个epoch：

```
1. 打乱训练数据顺序
2. 对于每个训练对(center, context)：
   a. 准备输入张量
   b. 采样K个负样本
   c. 前向传播：
      - 获取v_c, u_o, u_neg
      - 计算positive_scores = u_o^T v_c
      - 计算negative_scores = u_{w_i}^T v_c
   d. 计算损失：
      - positive_loss = -log(σ(positive_scores))
      - negative_loss = -sum(log(σ(-negative_scores)))
      - total_loss = mean(positive_loss + negative_loss)
   e. 反向传播：
      - loss.backward()
      - optimizer.step()
   f. 累积损失用于监控
3. 打印平均损失
```

### 超参数设置

- **EMBEDDING_DIM = 32**：词向量维度
- **LEARNING_RATE = 0.01**：学习率
- **EPOCHS = 5**：训练轮数
- **K_NEGATIVE_SAMPLES = 5**：每个正样本对应的负样本数
- **WINDOW_SIZE = 5**：上下文窗口大小
- **优化器**：AdamW（自适应学习率）

## 关键实现细节

### 1. 批量处理

虽然每个训练对是独立处理的（batch_size=1），但模型设计支持批量处理：
- `center_words`: `(batch_size,)`
- `context_words`: `(batch_size,)`
- `negative_samples`: `(batch_size, k)`

### 2. 矩阵运算优化

- **正样本得分**：使用元素乘法和求和（`torch.sum(u_o * v_c, dim=1)`）
- **负样本得分**：使用批量矩阵乘法（`torch.bmm`）高效计算所有负样本的得分

### 3. 嵌入初始化

使用均匀分布初始化：
```python
uniform(-0.5/embedding_dim, 0.5/embedding_dim)
```
这是Word2Vec论文中的标准初始化方法。

### 4. 索引映射

`NegativeSampler`需要正确映射词到词汇表索引：
- 接收`word_to_ix`参数
- 将采样得到的词映射到正确的词汇表索引
- 确保负样本索引与模型嵌入层一致

## 训练监控

### 损失变化

- **初始损失**：约0.7-1.0（随机初始化）
- **训练过程**：损失应稳定下降
- **最终损失**：约0.1-0.3（取决于数据量和训练轮数）

### 进度显示

- 使用`tqdm`显示训练进度条
- 每1000个样本更新一次损失显示
- 每个epoch结束后打印平均损失

## 可视化

训练完成后，使用PCA将高维词向量降维到2D进行可视化：

1. 提取`center_embed`权重矩阵
2. 使用PCA降维到2维
3. 绘制散点图并标注每个词

**预期结果**：
- 语义相似的词应该聚集在一起
- 例如："king"和"queen"应该接近
- "prince"和"princess"应该接近

## 代码执行流程

```python
1. load_data() → 加载和预处理语料库
2. create_skipgram_data() → 生成训练对
3. Word2Vec() → 初始化模型
4. NegativeSampler() → 初始化负采样器
5. train() → 训练模型
   - 对每个训练对：
     - 采样负样本
     - 前向传播
     - 计算损失
     - 反向传播
     - 更新参数
6. visualize_embeddings() → 可视化词向量
```

## 数学原理总结

### Skip-Gram目标

给定中心词`c`，预测其上下文词`o`的概率：

$$P(o|c) = \frac{\exp(u_o^T v_c)}{\sum_{w \in V} \exp(u_w^T v_c)}$$

### 负采样近似

由于计算所有词的softmax太昂贵（O(V)），使用负采样近似：

- 只考虑1个正样本和K个负样本
- 将softmax问题转化为K+1个二分类问题
- 复杂度从O(V)降低到O(K)，其中K<<V

### 损失函数推导

负采样损失可以理解为：
- 最大化正样本对的概率：`P(positive=True | c, o) = σ(u_o^T v_c)`
- 最小化负样本对的概率：`P(positive=True | c, w_i) = σ(u_{w_i}^T v_c)`

使用对数似然：
$$\mathcal{L} = -\log P(\text{positive=True}|c,o) - \sum_{i=1}^K \log P(\text{positive=False}|c,w_i)$$

展开后得到Eq. (8)。

## 可能的改进

1. **批次训练**：当前是单样本训练，可以改为批量训练提高效率
2. **学习率调度**：使用学习率衰减策略
3. **子采样**：对高频词进行子采样（如"the", "a"）
4. **分层softmax**：替代负采样的另一种方法
5. **预训练初始化**：使用预训练词向量初始化

## 运行说明

### 安装依赖

```bash
pip install -r requirements.txt
```

### 运行训练

```bash
python word2vec.py
```

### 输出

1. 词汇表大小和token数量
2. 生成的训练对数量
3. 每个epoch的训练损失
4. 词向量可视化图（PCA降维后的2D散点图）

## 参考资料

1. Mikolov et al. (2013). "Efficient Estimation of Word Representations in Vector Space"
2. Mikolov et al. (2013). "Distributed Representations of Words and Phrases and their Compositionality"
3. Goldberg & Levy (2014). "word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method"

