# Word2Vec Skip-Gram 模型代码架构分析

## 📋 项目概述

这是一个使用 **PyTorch** 实现的 **Word2Vec Skip-Gram** 模型，目标是训练词向量（word embeddings），通过负采样（Negative Sampling）来优化训练过程。

## 🏗️ 代码架构分析

### 1. 整体结构

代码采用模块化设计，主要包含以下组件：

```
word2vec.py
├── 数据预处理模块
│   ├── load_data()          # 加载和预处理语料
│   └── create_skipgram_data() # 生成Skip-Gram训练数据
├── 模型定义模块
│   ├── Word2Vec (nn.Module)  # Word2Vec模型类 [待实现]
│   └── NegativeSampler       # 负采样器类 [待实现]
├── 训练模块
│   └── train()              # 训练循环 [部分待实现]
└── 可视化模块
    └── visualize_embeddings() # 使用PCA可视化词向量
```

### 2. 各模块详细分析

#### 2.1 数据预处理模块

**`load_data(corpus_path, min_freq=20)`**
- **功能**: 加载语料文件并进行预处理
- **处理流程**:
  1. 读取文本文件
  2. 转换为小写
  3. 移除非字母字符，保留空格
  4. 规范化空白字符
  5. 统计词频，过滤低频词（默认最低频率20）
  6. 构建词汇表（vocab），包含 `<unk>` 作为未知词标记
  7. 创建 `word_to_ix` 和 `ix_to_word` 映射字典
- **返回**: 
  - `tokens`: 处理后的词序列
  - `word_counts`: 词频统计
  - `word_to_ix`: 词到索引的映射
  - `ix_to_word`: 索引到词的映射
  - `unk_ix`: 未知词的索引

**`create_skipgram_data(tokens, window_size, word_to_ix)`**
- **功能**: 生成Skip-Gram模型的训练数据对
- **原理**: 对于每个中心词，收集其窗口内的上下文词
- **窗口大小**: 默认5（中心词前后各5个词）
- **返回**: `(center_word_idx, context_word_idx)` 对的列表

#### 2.2 模型定义模块

**`Word2Vec(nn.Module)`** ⚠️ **待实现**
- **TODO 1**: 需要实现两个嵌入层
  - `in_embed`: 输入嵌入层（中心词）
  - `out_embed`: 输出嵌入层（上下文词）
- **参数**:
  - `vocab_size`: 词汇表大小
  - `embedding_dim`: 词向量维度（代码中设置为32）
- **预期结构**:
  ```python
  class Word2Vec(nn.Module):
      def __init__(self, vocab_size, embedding_dim):
          # 定义两个嵌入层
          # in_embed: 用于中心词
          # out_embed: 用于上下文词
      
      def forward(self, center_word_idx, context_word_idx, neg_samples):
          # 前向传播
          # 返回中心词、上下文词和负样本的嵌入表示
  ```

**`NegativeSampler`** ⚠️ **待实现**
- **TODO 3**: 实现负采样器
- **功能**: 根据词频分布采样负样本（非上下文词）
- **参数**: `word_counts`（词频统计）
- **方法**: `get_negative_samples(context_word_idx, k)` 
  - 返回 k 个负样本索引
- **原理**: 通常使用词频的3/4次方进行采样（幂律分布）

#### 2.3 训练模块

**`train()`** ⚠️ **部分待实现**
- **已完成部分**:
  - 训练循环框架
  - 数据打乱
  - 进度条显示
  - 损失记录
- **TODO 4**: 需要完成实际训练逻辑
  - 前向传播
  - 计算损失
  - 反向传播
  - 参数更新

**损失函数** ⚠️ **待定义**
- **TODO 2**: 定义损失函数
- **预期**: 使用负采样损失（Negative Sampling Loss）
- **公式**: 
  ```
  Loss = -log(σ(center · context)) - Σ log(σ(-center · neg_sample))
  ```
  其中 σ 是 sigmoid 函数

#### 2.4 可视化模块

**`visualize_embeddings()`** ✅ **已完成**
- **功能**: 使用PCA将高维词向量降维到2D并可视化
- **工具**: sklearn的PCA + matplotlib
- **注意**: 代码中使用了 `model.in_embed.weight.data`，说明模型需要有 `in_embed` 属性

### 3. 主程序流程

```python
if __name__ == "__main__":
    1. 加载数据 (load_data)
    2. 生成训练数据对 (create_skipgram_data)
    3. 初始化模型 (Word2Vec)
    4. 定义优化器 (AdamW)
    5. 初始化负采样器 (NegativeSampler)
    6. 训练模型 (train)
    7. 可视化词向量 (visualize_embeddings)
```

### 4. 训练参数

- **EMBEDDING_DIM**: 32（词向量维度）
- **LEARNING_RATE**: 0.01（学习率）
- **EPOCHS**: 5（训练轮数）
- **K_NEGATIVE_SAMPLES**: 5（每个正样本对应的负样本数）
- **WINDOW_SIZE**: 5（Skip-Gram窗口大小）

## 🎯 训练目标

使用 **PyTorch** 训练一个 **Word2Vec Skip-Gram** 模型，目标包括：

1. **学习词向量表示**: 将词汇映射到32维的连续向量空间
2. **捕获语义关系**: 通过Skip-Gram架构学习词的上下文关系
3. **优化训练效率**: 使用负采样技术加速训练过程
4. **可视化结果**: 通过PCA降维可视化词向量的分布

## ✅ 已完成的功能

1. ✅ 数据加载和预处理
2. ✅ Skip-Gram训练数据生成
3. ✅ 训练循环框架
4. ✅ 可视化功能

## ⚠️ 需要补全的部分

### TODO 1: 实现 Word2Vec 模型类

需要实现的核心功能：
- 定义两个 `nn.Embedding` 层（`in_embed` 和 `out_embed`）
- 实现 `forward()` 方法，返回中心词、上下文词和负样本的嵌入

```python
class Word2Vec(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(Word2Vec, self).__init__()
        # 实现两个嵌入层
        self.in_embed = nn.Embedding(vocab_size, embedding_dim)
        self.out_embed = nn.Embedding(vocab_size, embedding_dim)
    
    def forward(self, center_idx, context_idx, neg_samples):
        # 返回嵌入向量
        pass
```

### TODO 2: 定义损失函数

需要实现负采样损失函数：
- 使用 sigmoid 函数
- 计算正样本对的概率
- 计算负样本对的概率
- 组合成最终损失

### TODO 3: 实现 NegativeSampler 类

需要实现：
- 根据词频构建采样分布（通常使用频率的3/4次方）
- `get_negative_samples()` 方法，返回k个负样本索引
- 确保负样本不包含上下文词本身

```python
class NegativeSampler:
    def __init__(self, word_counts):
        # 构建采样分布
        pass
    
    def get_negative_samples(self, context_word_idx, k):
        # 返回k个负样本索引
        pass
```

### TODO 4: 完成训练循环

在 `train()` 函数中实现：
- 前向传播：获取嵌入向量
- 计算损失：使用负采样损失
- 反向传播：`loss.backward()`
- 参数更新：`optimizer.step()`
- 梯度清零：`optimizer.zero_grad()`

## 📝 补充说明

### 数据特点

- **语料库**: `corpus.txt` 包含关于国王、王后、王子、公主的简单英文文本
- **预期效果**: 训练后，语义相近的词（如 king-queen, prince-princess）应该在向量空间中距离较近

### 技术要点

1. **Skip-Gram架构**: 给定中心词，预测上下文词
2. **负采样**: 只更新一小部分负样本，而不是整个词汇表，大幅提升训练效率
3. **嵌入层**: 使用两个独立的嵌入层分别表示输入和输出

### 潜在问题

1. **第96行**: `torch.cuda.get_device_name(0)` 可能在CPU环境下报错，建议修改为：
   ```python
   print("PyTorch version:", torch.__version__)
   print("CUDA available:", torch.cuda.is_available())
   if torch.cuda.is_available():
       print("CUDA device:", torch.cuda.get_device_name(0))
   ```

2. **min_freq参数**: 第99行设置为1，与函数默认值20不一致，可能导致词汇表过大

3. **损失函数未定义**: 第113行 `loss_fn = None`，需要实现后才能训练

## 🚀 下一步行动

1. 实现 `Word2Vec` 类的 `__init__` 和 `forward` 方法
2. 实现 `NegativeSampler` 类
3. 定义负采样损失函数
4. 完成 `train()` 函数中的训练逻辑
5. 修复第96行的CUDA检查代码
6. 运行训练并检查结果

---

**生成时间**: 代码分析完成
**代码行数**: 121行
**待完成任务**: 4个主要TODO项

